----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 32, 300, 300]           2,624
       BatchNorm2d-2         [-1, 32, 300, 300]              64
            Conv2d-3         [-1, 32, 150, 150]             288
       BatchNorm2d-4         [-1, 32, 150, 150]              64
         Hardswish-5         [-1, 32, 150, 150]               0
       ConvBNLayer-6         [-1, 32, 150, 150]               0
            Conv2d-7         [-1, 64, 150, 150]           2,048
       BatchNorm2d-8         [-1, 64, 150, 150]             128
         Hardswish-9         [-1, 64, 150, 150]               0
      ConvBNLayer-10         [-1, 64, 150, 150]               0
DepthwiseSeparable-11         [-1, 64, 150, 150]               0
           Conv2d-12           [-1, 64, 75, 75]             576
      BatchNorm2d-13           [-1, 64, 75, 75]             128
        Hardswish-14           [-1, 64, 75, 75]               0
      ConvBNLayer-15           [-1, 64, 75, 75]               0
           Conv2d-16          [-1, 128, 75, 75]           8,192
      BatchNorm2d-17          [-1, 128, 75, 75]             256
        Hardswish-18          [-1, 128, 75, 75]               0
      ConvBNLayer-19          [-1, 128, 75, 75]               0
DepthwiseSeparable-20          [-1, 128, 75, 75]               0
           Conv2d-21          [-1, 128, 75, 75]         147,584
      BatchNorm2d-22          [-1, 128, 75, 75]             256
           Conv2d-23          [-1, 128, 75, 75]         147,584
      BatchNorm2d-24          [-1, 128, 75, 75]             256
AdaptiveAvgPool2d-25            [-1, 128, 1, 1]               0
           Conv1d-26               [-1, 1, 128]               5
          Sigmoid-27            [-1, 128, 1, 1]               0
        eca_block-28          [-1, 128, 75, 75]               0
    ResidualBlock-29          [-1, 128, 75, 75]               0
           Conv2d-30          [-1, 128, 75, 75]         147,584
      BatchNorm2d-31          [-1, 128, 75, 75]             256
           Conv2d-32          [-1, 128, 75, 75]         147,584
      BatchNorm2d-33          [-1, 128, 75, 75]             256
AdaptiveAvgPool2d-34            [-1, 128, 1, 1]               0
           Conv1d-35               [-1, 1, 128]               5
          Sigmoid-36            [-1, 128, 1, 1]               0
        eca_block-37          [-1, 128, 75, 75]               0
    ResidualBlock-38          [-1, 128, 75, 75]               0
           Conv2d-39          [-1, 128, 75, 75]         147,584
      BatchNorm2d-40          [-1, 128, 75, 75]             256
           Conv2d-41          [-1, 128, 75, 75]         147,584
      BatchNorm2d-42          [-1, 128, 75, 75]             256
AdaptiveAvgPool2d-43            [-1, 128, 1, 1]               0
           Conv1d-44               [-1, 1, 128]               5
          Sigmoid-45            [-1, 128, 1, 1]               0
        eca_block-46          [-1, 128, 75, 75]               0
    ResidualBlock-47          [-1, 128, 75, 75]               0
           Conv2d-48          [-1, 128, 75, 75]         147,584
      BatchNorm2d-49          [-1, 128, 75, 75]             256
           Conv2d-50          [-1, 128, 75, 75]         147,584
      BatchNorm2d-51          [-1, 128, 75, 75]             256
AdaptiveAvgPool2d-52            [-1, 128, 1, 1]               0
           Conv1d-53               [-1, 1, 128]               5
          Sigmoid-54            [-1, 128, 1, 1]               0
        eca_block-55          [-1, 128, 75, 75]               0
    ResidualBlock-56          [-1, 128, 75, 75]               0
           Conv2d-57          [-1, 128, 75, 75]         147,584
      BatchNorm2d-58          [-1, 128, 75, 75]             256
           Conv2d-59          [-1, 128, 75, 75]         147,584
      BatchNorm2d-60          [-1, 128, 75, 75]             256
AdaptiveAvgPool2d-61            [-1, 128, 1, 1]               0
           Conv1d-62               [-1, 1, 128]               5
          Sigmoid-63            [-1, 128, 1, 1]               0
        eca_block-64          [-1, 128, 75, 75]               0
    ResidualBlock-65          [-1, 128, 75, 75]               0
           Conv2d-66           [-1, 64, 75, 75]          16,384
      BatchNorm2d-67           [-1, 64, 75, 75]             128
      ConvBNLayer-68           [-1, 64, 75, 75]               0
           Conv2d-69           [-1, 64, 75, 75]          16,384
      BatchNorm2d-70           [-1, 64, 75, 75]             128
      ConvBNLayer-71           [-1, 64, 75, 75]               0
           Conv2d-72           [-1, 64, 75, 75]           4,096
      BatchNorm2d-73           [-1, 64, 75, 75]             128
      ConvBNLayer-74           [-1, 64, 75, 75]               0
           Conv2d-75           [-1, 64, 75, 75]          36,864
      BatchNorm2d-76           [-1, 64, 75, 75]             128
      ConvBNLayer-77           [-1, 64, 75, 75]               0
DarknetBottleneck-78           [-1, 64, 75, 75]               0
           Conv2d-79          [-1, 128, 75, 75]          16,384
      BatchNorm2d-80          [-1, 128, 75, 75]             256
      ConvBNLayer-81          [-1, 128, 75, 75]               0
         CSPLayer-82          [-1, 128, 75, 75]               0
AdaptiveAvgPool2d-83            [-1, 128, 1, 1]               0
           Conv2d-84             [-1, 16, 1, 1]           2,048
             ReLU-85             [-1, 16, 1, 1]               0
           Conv2d-86            [-1, 128, 1, 1]           2,048
AdaptiveMaxPool2d-87            [-1, 128, 1, 1]               0
           Conv2d-88             [-1, 16, 1, 1]           2,048
             ReLU-89             [-1, 16, 1, 1]               0
           Conv2d-90            [-1, 128, 1, 1]           2,048
          Sigmoid-91            [-1, 128, 1, 1]               0
 ChannelAttention-92            [-1, 128, 1, 1]               0
           Conv2d-93            [-1, 1, 75, 75]              18
          Sigmoid-94            [-1, 1, 75, 75]               0
 SpatialAttention-95            [-1, 1, 75, 75]               0
       cbam_block-96          [-1, 128, 75, 75]               0
  ConvTranspose2d-97         [-1, 64, 150, 150]         131,136
      BatchNorm2d-98         [-1, 64, 150, 150]             128
           Conv2d-99         [-1, 32, 150, 150]           4,096
     BatchNorm2d-100         [-1, 32, 150, 150]              64
     ConvBNLayer-101         [-1, 32, 150, 150]               0
          Conv2d-102         [-1, 32, 150, 150]           4,096
     BatchNorm2d-103         [-1, 32, 150, 150]              64
     ConvBNLayer-104         [-1, 32, 150, 150]               0
          Conv2d-105         [-1, 32, 150, 150]           1,024
     BatchNorm2d-106         [-1, 32, 150, 150]              64
     ConvBNLayer-107         [-1, 32, 150, 150]               0
          Conv2d-108         [-1, 32, 150, 150]           9,216
     BatchNorm2d-109         [-1, 32, 150, 150]              64
     ConvBNLayer-110         [-1, 32, 150, 150]               0
DarknetBottleneck-111         [-1, 32, 150, 150]               0
          Conv2d-112         [-1, 64, 150, 150]           4,096
     BatchNorm2d-113         [-1, 64, 150, 150]             128
     ConvBNLayer-114         [-1, 64, 150, 150]               0
        CSPLayer-115         [-1, 64, 150, 150]               0
AdaptiveAvgPool2d-116             [-1, 64, 1, 1]               0
          Conv2d-117              [-1, 8, 1, 1]             512
            ReLU-118              [-1, 8, 1, 1]               0
          Conv2d-119             [-1, 64, 1, 1]             512
AdaptiveMaxPool2d-120             [-1, 64, 1, 1]               0
          Conv2d-121              [-1, 8, 1, 1]             512
            ReLU-122              [-1, 8, 1, 1]               0
          Conv2d-123             [-1, 64, 1, 1]             512
         Sigmoid-124             [-1, 64, 1, 1]               0
ChannelAttention-125             [-1, 64, 1, 1]               0
          Conv2d-126          [-1, 1, 150, 150]              18
         Sigmoid-127          [-1, 1, 150, 150]               0
SpatialAttention-128          [-1, 1, 150, 150]               0
      cbam_block-129         [-1, 64, 150, 150]               0
 ConvTranspose2d-130         [-1, 32, 300, 300]          32,800
     BatchNorm2d-131         [-1, 32, 300, 300]              64
          Conv2d-132         [-1, 16, 300, 300]           1,024
     BatchNorm2d-133         [-1, 16, 300, 300]              32
     ConvBNLayer-134         [-1, 16, 300, 300]               0
          Conv2d-135         [-1, 16, 300, 300]           1,024
     BatchNorm2d-136         [-1, 16, 300, 300]              32
     ConvBNLayer-137         [-1, 16, 300, 300]               0
          Conv2d-138         [-1, 16, 300, 300]             256
     BatchNorm2d-139         [-1, 16, 300, 300]              32
     ConvBNLayer-140         [-1, 16, 300, 300]               0
          Conv2d-141         [-1, 16, 300, 300]           2,304
     BatchNorm2d-142         [-1, 16, 300, 300]              32
     ConvBNLayer-143         [-1, 16, 300, 300]               0
DarknetBottleneck-144         [-1, 16, 300, 300]               0
          Conv2d-145         [-1, 32, 300, 300]           1,024
     BatchNorm2d-146         [-1, 32, 300, 300]              64
     ConvBNLayer-147         [-1, 32, 300, 300]               0
        CSPLayer-148         [-1, 32, 300, 300]               0
AdaptiveAvgPool2d-149             [-1, 32, 1, 1]               0
          Conv2d-150              [-1, 4, 1, 1]             128
            ReLU-151              [-1, 4, 1, 1]               0
          Conv2d-152             [-1, 32, 1, 1]             128
AdaptiveMaxPool2d-153             [-1, 32, 1, 1]               0
          Conv2d-154              [-1, 4, 1, 1]             128
            ReLU-155              [-1, 4, 1, 1]               0
          Conv2d-156             [-1, 32, 1, 1]             128
         Sigmoid-157             [-1, 32, 1, 1]               0
ChannelAttention-158             [-1, 32, 1, 1]               0
          Conv2d-159          [-1, 1, 300, 300]              18
         Sigmoid-160          [-1, 1, 300, 300]               0
SpatialAttention-161          [-1, 1, 300, 300]               0
      cbam_block-162         [-1, 32, 300, 300]               0
 ConvTranspose2d-163         [-1, 32, 300, 300]          82,976
          Conv2d-164          [-1, 1, 300, 300]              33
          Conv2d-165          [-1, 1, 300, 300]              33
          Conv2d-166          [-1, 1, 300, 300]              33
          Conv2d-167          [-1, 1, 300, 300]              33
================================================================
Total params: 1,870,451
Trainable params: 1,870,451
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.34
Forward/backward pass size (MB): 859.66
Params size (MB): 7.14
Estimated Total Size (MB): 867.14
----------------------------------------------------------------


import torch.nn as nn
import torch
import torch.nn.functional as F
from torch.nn.parameter import Parameter
from inference.models.pico_det import CSPLayer
from inference.models.grasp_model import GraspModel, ResidualBlock
from inference.models.pp_lcnet import DepthwiseSeparable

class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=8):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        # 利用1x1卷积代替全连接
        self.fc1   = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)
        self.relu1 = nn.ReLU()
        self.fc2   = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        out = avg_out + max_out
        return self.sigmoid(out)

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()

        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'
        padding = 3 if kernel_size == 7 else 1
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)

class cbam_block(nn.Module):
    def __init__(self, channel, ratio=8, kernel_size=3):
        super(cbam_block, self).__init__()
        self.channelattention = ChannelAttention(channel, ratio=ratio)
        self.spatialattention = SpatialAttention(kernel_size=kernel_size)

    def forward(self, x):
        x0 = self.channelattention(x)
        x1 = self.spatialattention(x)
        x = x0 * x * x1
        return x

class sa_layer(nn.Module):
    """Constructs a Channel Spatial Group module.
    Args:
        k_size: Adaptive selection of kernel size
    """

    def __init__(self, channel, groups=64):
        super(sa_layer, self).__init__()
        self.groups = groups
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.cweight = Parameter(torch.zeros(1, channel // (2 * groups), 1, 1))
        self.cbias = Parameter(torch.ones(1, channel // (2 * groups), 1, 1))
        self.sweight = Parameter(torch.zeros(1, channel // (2 * groups), 1, 1))
        self.sbias = Parameter(torch.ones(1, channel // (2 * groups), 1, 1))

        self.sigmoid = nn.Sigmoid()
        self.gn = nn.GroupNorm(channel // (2 * groups), channel // (2 * groups))

    @staticmethod
    def channel_shuffle(x, groups):
        b, c, h, w = x.shape

        x = x.reshape(b, groups, -1, h, w)
        x = x.permute(0, 2, 1, 3, 4)

        # flatten
        x = x.reshape(b, -1, h, w)

        return x

    def forward(self, x):
        b, c, h, w = x.shape
        print("shape x {}".format(x.shape))
        x = x.reshape(b * self.groups, -1, h, w)
        print("shape x {}".format(x.shape))
        x_0, x_1 = x.chunk(2, dim=1)

        # channel attention
        xn = self.avg_pool(x_0)
        xn = self.cweight * xn + self.cbias
        xn = x_0 * self.sigmoid(xn)

        # spatial attention
        xs = self.gn(x_1)
        xs = self.sweight * xs + self.sbias
        xs = x_1 * self.sigmoid(xs)

        # concatenate along channel axis
        out = torch.cat([xn, xs], dim=1)
        out = out.reshape(b, -1, h, w)

        out = self.channel_shuffle(out, 2)
        return out


class GenerativeResnet(GraspModel):

    def __init__(self, input_channels=4, output_channels=1, channel_size=32, dropout=False, prob=0.0):
        super(GenerativeResnet, self).__init__()
        
        self.conv1 = nn.Conv2d(input_channels, channel_size, kernel_size=9, stride=1, padding=4)  #32 224
        # self.conv1 = DepthwiseSeparable(input_channels,channel_size,stride = 1,use_se=True)
        self.bn1 = nn.BatchNorm2d(channel_size)

        # self.conv2 = nn.Conv2d(channel_size, channel_size * 2, kernel_size=4, stride=2, padding=1) #64 112
        self.conv2 = DepthwiseSeparable(channel_size,channel_size * 2,stride = 2,use_se=False)
        self.bn2 = nn.BatchNorm2d(channel_size * 2)

        # self.conv3 = nn.Conv2d(channel_size * 2, channel_size * 4, kernel_size=4, stride=2, padding=1) #128 64
        self.conv3 = DepthwiseSeparable(channel_size * 2,channel_size * 4,stride = 2,use_se=False)
        self.bn3 = nn.BatchNorm2d(channel_size * 4)

        # self.res1 = DepthwiseSeparable(channel_size * 4, channel_size * 4,dw_size=5,use_se=True)
        # self.res2 = DepthwiseSeparable(channel_size * 4, channel_size * 4,dw_size=5,use_se=True)
        # self.res3 = DepthwiseSeparable(channel_size * 4, channel_size * 4,dw_size=5,use_se=True)
        # self.res4 = DepthwiseSeparable(channel_size * 4, channel_size * 4,dw_size=5,use_se=True)
        # self.res5 = DepthwiseSeparable(channel_size * 4, channel_size * 4,dw_size=5,use_se=True)

        self.res1 = ResidualBlock(channel_size * 4, channel_size * 4, use_att=True) #128 56
        self.res2 = ResidualBlock(channel_size * 4, channel_size * 4, use_att=True) #128 56
        self.res3 = ResidualBlock(channel_size * 4, channel_size * 4, use_att=True) #128 56
        self.res4 = ResidualBlock(channel_size * 4, channel_size * 4, use_att=True) #128 56
        self.res5 = ResidualBlock(channel_size * 4, channel_size * 4, use_att=True) #128 56
        
        self.csp_layer_1 = CSPLayer(channel_size * 4 + channel_size * 4,channel_size * 4)
        self.sa_layer_1 = cbam_block(channel_size * 4)
        self.conv4 = nn.ConvTranspose2d(channel_size * 4, channel_size * 2, kernel_size=4, stride=2, padding=1) 
        self.bn4 = nn.BatchNorm2d(channel_size * 2)
        self.csp_layer_2 = CSPLayer(channel_size * 2 + channel_size * 2,channel_size*2)
        self.sa_layer_2 = cbam_block(channel_size * 2)
        self.conv5 = nn.ConvTranspose2d(channel_size * 2, channel_size, kernel_size=4, stride=2, padding=1)
        self.bn5 = nn.BatchNorm2d(channel_size)

        self.csp_layer_3 = CSPLayer(channel_size  + channel_size ,channel_size )
        self.sa_layer_3 = cbam_block(channel_size * 1)
        self.conv6 = nn.ConvTranspose2d(channel_size, channel_size, kernel_size=9, stride=1, padding=4)

        self.pos_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=1)
        self.cos_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=1)
        self.sin_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=1)
        self.width_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=1)

        self.dropout = dropout
        self.dropout_pos = nn.Dropout(p=prob)
        self.dropout_cos = nn.Dropout(p=prob)
        self.dropout_sin = nn.Dropout(p=prob)
        self.dropout_wid = nn.Dropout(p=prob)

        for m in self.modules():
            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
                nn.init.xavier_uniform_(m.weight, gain=1)
        
    def forward(self, x_in):
        dbg=0
        # x1 = self.conv1(x_in)
        x1 = F.hardswish(self.bn1(self.conv1(x_in))) #32
        if dbg == 1:
            print('x1.shape  {}'.format(x1.shape))
        x2 = self.conv2(x1)
        if dbg == 1:
            print('x2.shape  {}'.format(x2.shape))
        x3 = self.conv3(x2)
        if dbg == 1:
            print('x3.shape  {}'.format(x3.shape))
        xx = self.res1(x3)
        xx = self.res2(xx)
        xx = self.res3(xx)
        xx = self.res4(xx)
        x4 = self.res5(xx)
        if dbg == 1:
            print('xx.shape  {}'.format(xx.shape))

        x43 = self.csp_layer_1(torch.cat((x4, x3), dim=1))  #128+128 128
        if dbg == 1:
            print('csp x43.shape  {}'.format(x43.shape))
        x43 = self.sa_layer_1(x43)
        if dbg == 1:
            print('cbam x43.shape  {}'.format(x43.shape))
        x43 = F.hardswish(self.bn4(self.conv4(x43)))
        if dbg == 1:
            print('x43.shape  {}'.format(x43.shape))

        x42 = self.csp_layer_2(torch.cat((x43, x2), dim=1))  #64+64 64
        if dbg == 1:
            print('csp x42.shape  {}'.format(x42.shape))
        x42 = self.sa_layer_2(x42)
        if dbg == 1:
            print('cbam x42.shape  {}'.format(x42.shape))
        x42 = F.hardswish(self.bn5(self.conv5(x42)))
        
        if dbg == 1:
            print('x42.shape  {}'.format(x42.shape))

        x41 = self.csp_layer_3(torch.cat((x42, x1), dim=1))  #32+32 32
        x41 = self.sa_layer_3(x41)
        if dbg == 1:
            print('csp x41.shape  {}'.format(x41.shape))
        x = self.conv6(x41)
        if dbg == 1:
            print('x.shape  {}'.format(x.shape))
        if self.dropout:
            pos_output = self.pos_output(self.dropout_pos(x))
            cos_output = self.cos_output(self.dropout_cos(x))
            sin_output = self.sin_output(self.dropout_sin(x))
            width_output = self.width_output(self.dropout_wid(x))
        else:
            pos_output = self.pos_output(x)
            cos_output = self.cos_output(x)
            sin_output = self.sin_output(x)
            width_output = self.width_output(x)
        if dbg == 1:
            print('pos_output.shape  {}'.format(pos_output.shape))
        return pos_output, cos_output, sin_output, width_output
if __name__ == "__main__":
    model = GenerativeResnet()
    model.eval()
    input = torch.rand(1, 4, 224, 224)
    output = model(input)
