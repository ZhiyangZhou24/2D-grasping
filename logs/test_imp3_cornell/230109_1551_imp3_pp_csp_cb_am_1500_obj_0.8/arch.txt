----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1          [-1, 4, 224, 224]              36
       BatchNorm2d-2          [-1, 4, 224, 224]               8
         Hardswish-3          [-1, 4, 224, 224]               0
       ConvBNLayer-4          [-1, 4, 224, 224]               0
 AdaptiveAvgPool2d-5              [-1, 4, 1, 1]               0
            Conv2d-6              [-1, 1, 1, 1]               5
              ReLU-7              [-1, 1, 1, 1]               0
            Conv2d-8              [-1, 4, 1, 1]               8
       Hardsigmoid-9              [-1, 4, 1, 1]               0
         SEModule-10          [-1, 4, 224, 224]               0
           Conv2d-11         [-1, 32, 224, 224]             128
      BatchNorm2d-12         [-1, 32, 224, 224]              64
        Hardswish-13         [-1, 32, 224, 224]               0
      ConvBNLayer-14         [-1, 32, 224, 224]               0
DepthwiseSeparable-15         [-1, 32, 224, 224]               0
           Conv2d-16         [-1, 32, 112, 112]             288
      BatchNorm2d-17         [-1, 32, 112, 112]              64
        Hardswish-18         [-1, 32, 112, 112]               0
      ConvBNLayer-19         [-1, 32, 112, 112]               0
AdaptiveAvgPool2d-20             [-1, 32, 1, 1]               0
           Conv2d-21              [-1, 8, 1, 1]             264
             ReLU-22              [-1, 8, 1, 1]               0
           Conv2d-23             [-1, 32, 1, 1]             288
      Hardsigmoid-24             [-1, 32, 1, 1]               0
         SEModule-25         [-1, 32, 112, 112]               0
           Conv2d-26         [-1, 64, 112, 112]           2,048
      BatchNorm2d-27         [-1, 64, 112, 112]             128
        Hardswish-28         [-1, 64, 112, 112]               0
      ConvBNLayer-29         [-1, 64, 112, 112]               0
DepthwiseSeparable-30         [-1, 64, 112, 112]               0
           Conv2d-31           [-1, 64, 56, 56]             576
      BatchNorm2d-32           [-1, 64, 56, 56]             128
        Hardswish-33           [-1, 64, 56, 56]               0
      ConvBNLayer-34           [-1, 64, 56, 56]               0
AdaptiveAvgPool2d-35             [-1, 64, 1, 1]               0
           Conv2d-36             [-1, 16, 1, 1]           1,040
             ReLU-37             [-1, 16, 1, 1]               0
           Conv2d-38             [-1, 64, 1, 1]           1,088
      Hardsigmoid-39             [-1, 64, 1, 1]               0
         SEModule-40           [-1, 64, 56, 56]               0
           Conv2d-41          [-1, 128, 56, 56]           8,192
      BatchNorm2d-42          [-1, 128, 56, 56]             256
        Hardswish-43          [-1, 128, 56, 56]               0
      ConvBNLayer-44          [-1, 128, 56, 56]               0
DepthwiseSeparable-45          [-1, 128, 56, 56]               0
           Conv2d-46          [-1, 128, 56, 56]         147,584
      BatchNorm2d-47          [-1, 128, 56, 56]             256
           Conv2d-48          [-1, 128, 56, 56]         147,584
      BatchNorm2d-49          [-1, 128, 56, 56]             256
    ResidualBlock-50          [-1, 128, 56, 56]               0
           Conv2d-51          [-1, 128, 56, 56]         147,584
      BatchNorm2d-52          [-1, 128, 56, 56]             256
           Conv2d-53          [-1, 128, 56, 56]         147,584
      BatchNorm2d-54          [-1, 128, 56, 56]             256
    ResidualBlock-55          [-1, 128, 56, 56]               0
           Conv2d-56          [-1, 128, 56, 56]         147,584
      BatchNorm2d-57          [-1, 128, 56, 56]             256
           Conv2d-58          [-1, 128, 56, 56]         147,584
      BatchNorm2d-59          [-1, 128, 56, 56]             256
    ResidualBlock-60          [-1, 128, 56, 56]               0
           Conv2d-61          [-1, 128, 56, 56]         147,584
      BatchNorm2d-62          [-1, 128, 56, 56]             256
           Conv2d-63          [-1, 128, 56, 56]         147,584
      BatchNorm2d-64          [-1, 128, 56, 56]             256
    ResidualBlock-65          [-1, 128, 56, 56]               0
           Conv2d-66          [-1, 128, 56, 56]         147,584
      BatchNorm2d-67          [-1, 128, 56, 56]             256
           Conv2d-68          [-1, 128, 56, 56]         147,584
      BatchNorm2d-69          [-1, 128, 56, 56]             256
    ResidualBlock-70          [-1, 128, 56, 56]               0
           Conv2d-71           [-1, 64, 56, 56]          16,384
      BatchNorm2d-72           [-1, 64, 56, 56]             128
      ConvBNLayer-73           [-1, 64, 56, 56]               0
           Conv2d-74           [-1, 64, 56, 56]          16,384
      BatchNorm2d-75           [-1, 64, 56, 56]             128
      ConvBNLayer-76           [-1, 64, 56, 56]               0
           Conv2d-77           [-1, 64, 56, 56]           4,096
      BatchNorm2d-78           [-1, 64, 56, 56]             128
      ConvBNLayer-79           [-1, 64, 56, 56]               0
           Conv2d-80           [-1, 64, 56, 56]          36,864
      BatchNorm2d-81           [-1, 64, 56, 56]             128
      ConvBNLayer-82           [-1, 64, 56, 56]               0
DarknetBottleneck-83           [-1, 64, 56, 56]               0
           Conv2d-84          [-1, 128, 56, 56]          16,384
      BatchNorm2d-85          [-1, 128, 56, 56]             256
      ConvBNLayer-86          [-1, 128, 56, 56]               0
         CSPLayer-87          [-1, 128, 56, 56]               0
AdaptiveAvgPool2d-88            [-1, 128, 1, 1]               0
           Conv2d-89             [-1, 16, 1, 1]           2,048
             ReLU-90             [-1, 16, 1, 1]               0
           Conv2d-91            [-1, 128, 1, 1]           2,048
AdaptiveMaxPool2d-92            [-1, 128, 1, 1]               0
           Conv2d-93             [-1, 16, 1, 1]           2,048
             ReLU-94             [-1, 16, 1, 1]               0
           Conv2d-95            [-1, 128, 1, 1]           2,048
          Sigmoid-96            [-1, 128, 1, 1]               0
 ChannelAttention-97            [-1, 128, 1, 1]               0
           Conv2d-98            [-1, 1, 56, 56]              18
          Sigmoid-99            [-1, 1, 56, 56]               0
SpatialAttention-100            [-1, 1, 56, 56]               0
      cbam_block-101          [-1, 128, 56, 56]               0
 ConvTranspose2d-102         [-1, 64, 112, 112]         131,136
     BatchNorm2d-103         [-1, 64, 112, 112]             128
          Conv2d-104         [-1, 32, 112, 112]           4,096
     BatchNorm2d-105         [-1, 32, 112, 112]              64
     ConvBNLayer-106         [-1, 32, 112, 112]               0
          Conv2d-107         [-1, 32, 112, 112]           4,096
     BatchNorm2d-108         [-1, 32, 112, 112]              64
     ConvBNLayer-109         [-1, 32, 112, 112]               0
          Conv2d-110         [-1, 32, 112, 112]           1,024
     BatchNorm2d-111         [-1, 32, 112, 112]              64
     ConvBNLayer-112         [-1, 32, 112, 112]               0
          Conv2d-113         [-1, 32, 112, 112]           9,216
     BatchNorm2d-114         [-1, 32, 112, 112]              64
     ConvBNLayer-115         [-1, 32, 112, 112]               0
DarknetBottleneck-116         [-1, 32, 112, 112]               0
          Conv2d-117         [-1, 64, 112, 112]           4,096
     BatchNorm2d-118         [-1, 64, 112, 112]             128
     ConvBNLayer-119         [-1, 64, 112, 112]               0
        CSPLayer-120         [-1, 64, 112, 112]               0
AdaptiveAvgPool2d-121             [-1, 64, 1, 1]               0
          Conv2d-122              [-1, 8, 1, 1]             512
            ReLU-123              [-1, 8, 1, 1]               0
          Conv2d-124             [-1, 64, 1, 1]             512
AdaptiveMaxPool2d-125             [-1, 64, 1, 1]               0
          Conv2d-126              [-1, 8, 1, 1]             512
            ReLU-127              [-1, 8, 1, 1]               0
          Conv2d-128             [-1, 64, 1, 1]             512
         Sigmoid-129             [-1, 64, 1, 1]               0
ChannelAttention-130             [-1, 64, 1, 1]               0
          Conv2d-131          [-1, 1, 112, 112]              18
         Sigmoid-132          [-1, 1, 112, 112]               0
SpatialAttention-133          [-1, 1, 112, 112]               0
      cbam_block-134         [-1, 64, 112, 112]               0
 ConvTranspose2d-135         [-1, 32, 224, 224]          32,800
     BatchNorm2d-136         [-1, 32, 224, 224]              64
          Conv2d-137         [-1, 16, 224, 224]           1,024
     BatchNorm2d-138         [-1, 16, 224, 224]              32
     ConvBNLayer-139         [-1, 16, 224, 224]               0
          Conv2d-140         [-1, 16, 224, 224]           1,024
     BatchNorm2d-141         [-1, 16, 224, 224]              32
     ConvBNLayer-142         [-1, 16, 224, 224]               0
          Conv2d-143         [-1, 16, 224, 224]             256
     BatchNorm2d-144         [-1, 16, 224, 224]              32
     ConvBNLayer-145         [-1, 16, 224, 224]               0
          Conv2d-146         [-1, 16, 224, 224]           2,304
     BatchNorm2d-147         [-1, 16, 224, 224]              32
     ConvBNLayer-148         [-1, 16, 224, 224]               0
DarknetBottleneck-149         [-1, 16, 224, 224]               0
          Conv2d-150         [-1, 32, 224, 224]           1,024
     BatchNorm2d-151         [-1, 32, 224, 224]              64
     ConvBNLayer-152         [-1, 32, 224, 224]               0
        CSPLayer-153         [-1, 32, 224, 224]               0
AdaptiveAvgPool2d-154             [-1, 32, 1, 1]               0
          Conv2d-155              [-1, 4, 1, 1]             128
            ReLU-156              [-1, 4, 1, 1]               0
          Conv2d-157             [-1, 32, 1, 1]             128
AdaptiveMaxPool2d-158             [-1, 32, 1, 1]               0
          Conv2d-159              [-1, 4, 1, 1]             128
            ReLU-160              [-1, 4, 1, 1]               0
          Conv2d-161             [-1, 32, 1, 1]             128
         Sigmoid-162             [-1, 32, 1, 1]               0
ChannelAttention-163             [-1, 32, 1, 1]               0
          Conv2d-164          [-1, 1, 224, 224]              18
         Sigmoid-165          [-1, 1, 224, 224]               0
SpatialAttention-166          [-1, 1, 224, 224]               0
      cbam_block-167         [-1, 32, 224, 224]               0
 ConvTranspose2d-168         [-1, 32, 224, 224]          82,976
         Dropout-169         [-1, 32, 224, 224]               0
          Conv2d-170          [-1, 1, 224, 224]              33
         Dropout-171         [-1, 32, 224, 224]               0
          Conv2d-172          [-1, 1, 224, 224]              33
         Dropout-173         [-1, 32, 224, 224]               0
          Conv2d-174          [-1, 1, 224, 224]              33
         Dropout-175         [-1, 32, 224, 224]               0
          Conv2d-176          [-1, 1, 224, 224]              33
================================================================
Total params: 1,870,667
Trainable params: 1,870,667
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.77
Forward/backward pass size (MB): 561.96
Params size (MB): 7.14
Estimated Total Size (MB): 569.86
----------------------------------------------------------------
