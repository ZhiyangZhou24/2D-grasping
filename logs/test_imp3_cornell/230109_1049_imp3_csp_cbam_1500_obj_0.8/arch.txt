----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 32, 224, 224]          10,400
       BatchNorm2d-2         [-1, 32, 224, 224]              64
            Conv2d-3         [-1, 64, 112, 112]          32,832
       BatchNorm2d-4         [-1, 64, 112, 112]             128
            Conv2d-5          [-1, 128, 56, 56]         131,200
       BatchNorm2d-6          [-1, 128, 56, 56]             256
            Conv2d-7          [-1, 128, 56, 56]         147,584
       BatchNorm2d-8          [-1, 128, 56, 56]             256
            Conv2d-9          [-1, 128, 56, 56]         147,584
      BatchNorm2d-10          [-1, 128, 56, 56]             256
    ResidualBlock-11          [-1, 128, 56, 56]               0
           Conv2d-12          [-1, 128, 56, 56]         147,584
      BatchNorm2d-13          [-1, 128, 56, 56]             256
           Conv2d-14          [-1, 128, 56, 56]         147,584
      BatchNorm2d-15          [-1, 128, 56, 56]             256
    ResidualBlock-16          [-1, 128, 56, 56]               0
           Conv2d-17          [-1, 128, 56, 56]         147,584
      BatchNorm2d-18          [-1, 128, 56, 56]             256
           Conv2d-19          [-1, 128, 56, 56]         147,584
      BatchNorm2d-20          [-1, 128, 56, 56]             256
    ResidualBlock-21          [-1, 128, 56, 56]               0
           Conv2d-22          [-1, 128, 56, 56]         147,584
      BatchNorm2d-23          [-1, 128, 56, 56]             256
           Conv2d-24          [-1, 128, 56, 56]         147,584
      BatchNorm2d-25          [-1, 128, 56, 56]             256
    ResidualBlock-26          [-1, 128, 56, 56]               0
           Conv2d-27          [-1, 128, 56, 56]         147,584
      BatchNorm2d-28          [-1, 128, 56, 56]             256
           Conv2d-29          [-1, 128, 56, 56]         147,584
      BatchNorm2d-30          [-1, 128, 56, 56]             256
    ResidualBlock-31          [-1, 128, 56, 56]               0
           Conv2d-32           [-1, 64, 56, 56]          16,384
      BatchNorm2d-33           [-1, 64, 56, 56]             128
      ConvBNLayer-34           [-1, 64, 56, 56]               0
           Conv2d-35           [-1, 64, 56, 56]          16,384
      BatchNorm2d-36           [-1, 64, 56, 56]             128
      ConvBNLayer-37           [-1, 64, 56, 56]               0
           Conv2d-38           [-1, 64, 56, 56]           4,096
      BatchNorm2d-39           [-1, 64, 56, 56]             128
      ConvBNLayer-40           [-1, 64, 56, 56]               0
           Conv2d-41           [-1, 64, 56, 56]          36,864
      BatchNorm2d-42           [-1, 64, 56, 56]             128
      ConvBNLayer-43           [-1, 64, 56, 56]               0
DarknetBottleneck-44           [-1, 64, 56, 56]               0
           Conv2d-45          [-1, 128, 56, 56]          16,384
      BatchNorm2d-46          [-1, 128, 56, 56]             256
      ConvBNLayer-47          [-1, 128, 56, 56]               0
         CSPLayer-48          [-1, 128, 56, 56]               0
AdaptiveAvgPool2d-49            [-1, 128, 1, 1]               0
           Conv2d-50             [-1, 16, 1, 1]           2,048
             ReLU-51             [-1, 16, 1, 1]               0
           Conv2d-52            [-1, 128, 1, 1]           2,048
AdaptiveMaxPool2d-53            [-1, 128, 1, 1]               0
           Conv2d-54             [-1, 16, 1, 1]           2,048
             ReLU-55             [-1, 16, 1, 1]               0
           Conv2d-56            [-1, 128, 1, 1]           2,048
          Sigmoid-57            [-1, 128, 1, 1]               0
 ChannelAttention-58            [-1, 128, 1, 1]               0
           Conv2d-59            [-1, 1, 56, 56]              18
          Sigmoid-60            [-1, 1, 56, 56]               0
 SpatialAttention-61            [-1, 1, 56, 56]               0
       cbam_block-62          [-1, 128, 56, 56]               0
  ConvTranspose2d-63         [-1, 64, 112, 112]         131,136
      BatchNorm2d-64         [-1, 64, 112, 112]             128
           Conv2d-65         [-1, 32, 112, 112]           4,096
      BatchNorm2d-66         [-1, 32, 112, 112]              64
      ConvBNLayer-67         [-1, 32, 112, 112]               0
           Conv2d-68         [-1, 32, 112, 112]           4,096
      BatchNorm2d-69         [-1, 32, 112, 112]              64
      ConvBNLayer-70         [-1, 32, 112, 112]               0
           Conv2d-71         [-1, 32, 112, 112]           1,024
      BatchNorm2d-72         [-1, 32, 112, 112]              64
      ConvBNLayer-73         [-1, 32, 112, 112]               0
           Conv2d-74         [-1, 32, 112, 112]           9,216
      BatchNorm2d-75         [-1, 32, 112, 112]              64
      ConvBNLayer-76         [-1, 32, 112, 112]               0
DarknetBottleneck-77         [-1, 32, 112, 112]               0
           Conv2d-78         [-1, 64, 112, 112]           4,096
      BatchNorm2d-79         [-1, 64, 112, 112]             128
      ConvBNLayer-80         [-1, 64, 112, 112]               0
         CSPLayer-81         [-1, 64, 112, 112]               0
AdaptiveAvgPool2d-82             [-1, 64, 1, 1]               0
           Conv2d-83              [-1, 8, 1, 1]             512
             ReLU-84              [-1, 8, 1, 1]               0
           Conv2d-85             [-1, 64, 1, 1]             512
AdaptiveMaxPool2d-86             [-1, 64, 1, 1]               0
           Conv2d-87              [-1, 8, 1, 1]             512
             ReLU-88              [-1, 8, 1, 1]               0
           Conv2d-89             [-1, 64, 1, 1]             512
          Sigmoid-90             [-1, 64, 1, 1]               0
 ChannelAttention-91             [-1, 64, 1, 1]               0
           Conv2d-92          [-1, 1, 112, 112]              18
          Sigmoid-93          [-1, 1, 112, 112]               0
 SpatialAttention-94          [-1, 1, 112, 112]               0
       cbam_block-95         [-1, 64, 112, 112]               0
  ConvTranspose2d-96         [-1, 32, 224, 224]          32,800
      BatchNorm2d-97         [-1, 32, 224, 224]              64
           Conv2d-98         [-1, 16, 224, 224]           1,024
      BatchNorm2d-99         [-1, 16, 224, 224]              32
     ConvBNLayer-100         [-1, 16, 224, 224]               0
          Conv2d-101         [-1, 16, 224, 224]           1,024
     BatchNorm2d-102         [-1, 16, 224, 224]              32
     ConvBNLayer-103         [-1, 16, 224, 224]               0
          Conv2d-104         [-1, 16, 224, 224]             256
     BatchNorm2d-105         [-1, 16, 224, 224]              32
     ConvBNLayer-106         [-1, 16, 224, 224]               0
          Conv2d-107         [-1, 16, 224, 224]           2,304
     BatchNorm2d-108         [-1, 16, 224, 224]              32
     ConvBNLayer-109         [-1, 16, 224, 224]               0
DarknetBottleneck-110         [-1, 16, 224, 224]               0
          Conv2d-111         [-1, 32, 224, 224]           1,024
     BatchNorm2d-112         [-1, 32, 224, 224]              64
     ConvBNLayer-113         [-1, 32, 224, 224]               0
        CSPLayer-114         [-1, 32, 224, 224]               0
AdaptiveAvgPool2d-115             [-1, 32, 1, 1]               0
          Conv2d-116              [-1, 4, 1, 1]             128
            ReLU-117              [-1, 4, 1, 1]               0
          Conv2d-118             [-1, 32, 1, 1]             128
AdaptiveMaxPool2d-119             [-1, 32, 1, 1]               0
          Conv2d-120              [-1, 4, 1, 1]             128
            ReLU-121              [-1, 4, 1, 1]               0
          Conv2d-122             [-1, 32, 1, 1]             128
         Sigmoid-123             [-1, 32, 1, 1]               0
ChannelAttention-124             [-1, 32, 1, 1]               0
          Conv2d-125          [-1, 1, 224, 224]              18
         Sigmoid-126          [-1, 1, 224, 224]               0
SpatialAttention-127          [-1, 1, 224, 224]               0
      cbam_block-128         [-1, 32, 224, 224]               0
 ConvTranspose2d-129         [-1, 32, 224, 224]          82,976
         Dropout-130         [-1, 32, 224, 224]               0
          Conv2d-131          [-1, 1, 224, 224]              33
         Dropout-132         [-1, 32, 224, 224]               0
          Conv2d-133          [-1, 1, 224, 224]              33
         Dropout-134         [-1, 32, 224, 224]               0
          Conv2d-135          [-1, 1, 224, 224]              33
         Dropout-136         [-1, 32, 224, 224]               0
          Conv2d-137          [-1, 1, 224, 224]              33
================================================================
Total params: 2,030,938
Trainable params: 2,030,938
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.77
Forward/backward pass size (MB): 467.02
Params size (MB): 7.75
Estimated Total Size (MB): 475.53
----------------------------------------------------------------
