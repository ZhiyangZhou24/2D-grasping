----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 32, 224, 224]          10,400
       BatchNorm2d-2         [-1, 32, 224, 224]              64
            Conv2d-3         [-1, 32, 112, 112]             288
       BatchNorm2d-4         [-1, 32, 112, 112]              64
         Hardswish-5         [-1, 32, 112, 112]               0
       ConvBNLayer-6         [-1, 32, 112, 112]               0
 AdaptiveAvgPool2d-7             [-1, 32, 1, 1]               0
            Conv2d-8              [-1, 8, 1, 1]             264
              ReLU-9              [-1, 8, 1, 1]               0
           Conv2d-10             [-1, 32, 1, 1]             288
      Hardsigmoid-11             [-1, 32, 1, 1]               0
         SEModule-12         [-1, 32, 112, 112]               0
           Conv2d-13         [-1, 64, 112, 112]           2,048
      BatchNorm2d-14         [-1, 64, 112, 112]             128
        Hardswish-15         [-1, 64, 112, 112]               0
      ConvBNLayer-16         [-1, 64, 112, 112]               0
DepthwiseSeparable-17         [-1, 64, 112, 112]               0
           Conv2d-18           [-1, 64, 56, 56]             576
      BatchNorm2d-19           [-1, 64, 56, 56]             128
        Hardswish-20           [-1, 64, 56, 56]               0
      ConvBNLayer-21           [-1, 64, 56, 56]               0
AdaptiveAvgPool2d-22             [-1, 64, 1, 1]               0
           Conv2d-23             [-1, 16, 1, 1]           1,040
             ReLU-24             [-1, 16, 1, 1]               0
           Conv2d-25             [-1, 64, 1, 1]           1,088
      Hardsigmoid-26             [-1, 64, 1, 1]               0
         SEModule-27           [-1, 64, 56, 56]               0
           Conv2d-28          [-1, 128, 56, 56]           8,192
      BatchNorm2d-29          [-1, 128, 56, 56]             256
        Hardswish-30          [-1, 128, 56, 56]               0
      ConvBNLayer-31          [-1, 128, 56, 56]               0
DepthwiseSeparable-32          [-1, 128, 56, 56]               0
           Conv2d-33          [-1, 128, 56, 56]         147,584
      BatchNorm2d-34          [-1, 128, 56, 56]             256
           Conv2d-35          [-1, 128, 56, 56]         147,584
      BatchNorm2d-36          [-1, 128, 56, 56]             256
    ResidualBlock-37          [-1, 128, 56, 56]               0
           Conv2d-38          [-1, 128, 56, 56]         147,584
      BatchNorm2d-39          [-1, 128, 56, 56]             256
           Conv2d-40          [-1, 128, 56, 56]         147,584
      BatchNorm2d-41          [-1, 128, 56, 56]             256
    ResidualBlock-42          [-1, 128, 56, 56]               0
           Conv2d-43          [-1, 128, 56, 56]         147,584
      BatchNorm2d-44          [-1, 128, 56, 56]             256
           Conv2d-45          [-1, 128, 56, 56]         147,584
      BatchNorm2d-46          [-1, 128, 56, 56]             256
    ResidualBlock-47          [-1, 128, 56, 56]               0
           Conv2d-48          [-1, 128, 56, 56]         147,584
      BatchNorm2d-49          [-1, 128, 56, 56]             256
           Conv2d-50          [-1, 128, 56, 56]         147,584
      BatchNorm2d-51          [-1, 128, 56, 56]             256
    ResidualBlock-52          [-1, 128, 56, 56]               0
           Conv2d-53          [-1, 128, 56, 56]         147,584
      BatchNorm2d-54          [-1, 128, 56, 56]             256
           Conv2d-55          [-1, 128, 56, 56]         147,584
      BatchNorm2d-56          [-1, 128, 56, 56]             256
    ResidualBlock-57          [-1, 128, 56, 56]               0
           Conv2d-58           [-1, 64, 56, 56]          16,384
      BatchNorm2d-59           [-1, 64, 56, 56]             128
      ConvBNLayer-60           [-1, 64, 56, 56]               0
           Conv2d-61           [-1, 64, 56, 56]          16,384
      BatchNorm2d-62           [-1, 64, 56, 56]             128
      ConvBNLayer-63           [-1, 64, 56, 56]               0
           Conv2d-64           [-1, 64, 56, 56]           4,096
      BatchNorm2d-65           [-1, 64, 56, 56]             128
      ConvBNLayer-66           [-1, 64, 56, 56]               0
           Conv2d-67           [-1, 64, 56, 56]          36,864
      BatchNorm2d-68           [-1, 64, 56, 56]             128
      ConvBNLayer-69           [-1, 64, 56, 56]               0
DarknetBottleneck-70           [-1, 64, 56, 56]               0
           Conv2d-71          [-1, 128, 56, 56]          16,384
      BatchNorm2d-72          [-1, 128, 56, 56]             256
      ConvBNLayer-73          [-1, 128, 56, 56]               0
         CSPLayer-74          [-1, 128, 56, 56]               0
AdaptiveAvgPool2d-75            [-1, 128, 1, 1]               0
           Conv2d-76             [-1, 16, 1, 1]           2,048
             ReLU-77             [-1, 16, 1, 1]               0
           Conv2d-78            [-1, 128, 1, 1]           2,048
AdaptiveMaxPool2d-79            [-1, 128, 1, 1]               0
           Conv2d-80             [-1, 16, 1, 1]           2,048
             ReLU-81             [-1, 16, 1, 1]               0
           Conv2d-82            [-1, 128, 1, 1]           2,048
          Sigmoid-83            [-1, 128, 1, 1]               0
 ChannelAttention-84            [-1, 128, 1, 1]               0
           Conv2d-85            [-1, 1, 56, 56]              18
          Sigmoid-86            [-1, 1, 56, 56]               0
 SpatialAttention-87            [-1, 1, 56, 56]               0
       cbam_block-88          [-1, 128, 56, 56]               0
  ConvTranspose2d-89         [-1, 64, 112, 112]         131,136
      BatchNorm2d-90         [-1, 64, 112, 112]             128
           Conv2d-91         [-1, 32, 112, 112]           4,096
      BatchNorm2d-92         [-1, 32, 112, 112]              64
      ConvBNLayer-93         [-1, 32, 112, 112]               0
           Conv2d-94         [-1, 32, 112, 112]           4,096
      BatchNorm2d-95         [-1, 32, 112, 112]              64
      ConvBNLayer-96         [-1, 32, 112, 112]               0
           Conv2d-97         [-1, 32, 112, 112]           1,024
      BatchNorm2d-98         [-1, 32, 112, 112]              64
      ConvBNLayer-99         [-1, 32, 112, 112]               0
          Conv2d-100         [-1, 32, 112, 112]           9,216
     BatchNorm2d-101         [-1, 32, 112, 112]              64
     ConvBNLayer-102         [-1, 32, 112, 112]               0
DarknetBottleneck-103         [-1, 32, 112, 112]               0
          Conv2d-104         [-1, 64, 112, 112]           4,096
     BatchNorm2d-105         [-1, 64, 112, 112]             128
     ConvBNLayer-106         [-1, 64, 112, 112]               0
        CSPLayer-107         [-1, 64, 112, 112]               0
AdaptiveAvgPool2d-108             [-1, 64, 1, 1]               0
          Conv2d-109              [-1, 8, 1, 1]             512
            ReLU-110              [-1, 8, 1, 1]               0
          Conv2d-111             [-1, 64, 1, 1]             512
AdaptiveMaxPool2d-112             [-1, 64, 1, 1]               0
          Conv2d-113              [-1, 8, 1, 1]             512
            ReLU-114              [-1, 8, 1, 1]               0
          Conv2d-115             [-1, 64, 1, 1]             512
         Sigmoid-116             [-1, 64, 1, 1]               0
ChannelAttention-117             [-1, 64, 1, 1]               0
          Conv2d-118          [-1, 1, 112, 112]              18
         Sigmoid-119          [-1, 1, 112, 112]               0
SpatialAttention-120          [-1, 1, 112, 112]               0
      cbam_block-121         [-1, 64, 112, 112]               0
 ConvTranspose2d-122         [-1, 32, 224, 224]          32,800
     BatchNorm2d-123         [-1, 32, 224, 224]              64
          Conv2d-124         [-1, 16, 224, 224]           1,024
     BatchNorm2d-125         [-1, 16, 224, 224]              32
     ConvBNLayer-126         [-1, 16, 224, 224]               0
          Conv2d-127         [-1, 16, 224, 224]           1,024
     BatchNorm2d-128         [-1, 16, 224, 224]              32
     ConvBNLayer-129         [-1, 16, 224, 224]               0
          Conv2d-130         [-1, 16, 224, 224]             256
     BatchNorm2d-131         [-1, 16, 224, 224]              32
     ConvBNLayer-132         [-1, 16, 224, 224]               0
          Conv2d-133         [-1, 16, 224, 224]           2,304
     BatchNorm2d-134         [-1, 16, 224, 224]              32
     ConvBNLayer-135         [-1, 16, 224, 224]               0
DarknetBottleneck-136         [-1, 16, 224, 224]               0
          Conv2d-137         [-1, 32, 224, 224]           1,024
     BatchNorm2d-138         [-1, 32, 224, 224]              64
     ConvBNLayer-139         [-1, 32, 224, 224]               0
        CSPLayer-140         [-1, 32, 224, 224]               0
AdaptiveAvgPool2d-141             [-1, 32, 1, 1]               0
          Conv2d-142              [-1, 4, 1, 1]             128
            ReLU-143              [-1, 4, 1, 1]               0
          Conv2d-144             [-1, 32, 1, 1]             128
AdaptiveMaxPool2d-145             [-1, 32, 1, 1]               0
          Conv2d-146              [-1, 4, 1, 1]             128
            ReLU-147              [-1, 4, 1, 1]               0
          Conv2d-148             [-1, 32, 1, 1]             128
         Sigmoid-149             [-1, 32, 1, 1]               0
ChannelAttention-150             [-1, 32, 1, 1]               0
          Conv2d-151          [-1, 1, 224, 224]              18
         Sigmoid-152          [-1, 1, 224, 224]               0
SpatialAttention-153          [-1, 1, 224, 224]               0
      cbam_block-154         [-1, 32, 224, 224]               0
 ConvTranspose2d-155         [-1, 32, 224, 224]          82,976
         Dropout-156         [-1, 32, 224, 224]               0
          Conv2d-157          [-1, 1, 224, 224]              33
         Dropout-158         [-1, 32, 224, 224]               0
          Conv2d-159          [-1, 1, 224, 224]              33
         Dropout-160         [-1, 32, 224, 224]               0
          Conv2d-161          [-1, 1, 224, 224]              33
         Dropout-162         [-1, 32, 224, 224]               0
          Conv2d-163          [-1, 1, 224, 224]              33
================================================================
Total params: 1,880,882
Trainable params: 1,880,882
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.77
Forward/backward pass size (MB): 517.55
Params size (MB): 7.17
Estimated Total Size (MB): 525.49
----------------------------------------------------------------
