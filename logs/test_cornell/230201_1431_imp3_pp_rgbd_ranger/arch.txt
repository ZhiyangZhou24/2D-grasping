----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 32, 224, 224]          10,400
       BatchNorm2d-2         [-1, 32, 224, 224]              64
            Conv2d-3         [-1, 32, 112, 112]             288
       BatchNorm2d-4         [-1, 32, 112, 112]              64
         Hardswish-5         [-1, 32, 112, 112]               0
       ConvBNLayer-6         [-1, 32, 112, 112]               0
            Conv2d-7         [-1, 64, 112, 112]           2,048
       BatchNorm2d-8         [-1, 64, 112, 112]             128
         Hardswish-9         [-1, 64, 112, 112]               0
      ConvBNLayer-10         [-1, 64, 112, 112]               0
DepthwiseSeparable-11         [-1, 64, 112, 112]               0
           Conv2d-12           [-1, 64, 56, 56]             576
      BatchNorm2d-13           [-1, 64, 56, 56]             128
        Hardswish-14           [-1, 64, 56, 56]               0
      ConvBNLayer-15           [-1, 64, 56, 56]               0
           Conv2d-16          [-1, 128, 56, 56]           8,192
      BatchNorm2d-17          [-1, 128, 56, 56]             256
        Hardswish-18          [-1, 128, 56, 56]               0
      ConvBNLayer-19          [-1, 128, 56, 56]               0
DepthwiseSeparable-20          [-1, 128, 56, 56]               0
           Conv2d-21          [-1, 128, 56, 56]         147,584
      BatchNorm2d-22          [-1, 128, 56, 56]             256
           Conv2d-23          [-1, 128, 56, 56]         147,584
      BatchNorm2d-24          [-1, 128, 56, 56]             256
AdaptiveAvgPool2d-25            [-1, 128, 1, 1]               0
           Conv1d-26               [-1, 1, 128]               5
          Sigmoid-27            [-1, 128, 1, 1]               0
        eca_block-28          [-1, 128, 56, 56]               0
    ResidualBlock-29          [-1, 128, 56, 56]               0
           Conv2d-30          [-1, 128, 56, 56]         147,584
      BatchNorm2d-31          [-1, 128, 56, 56]             256
           Conv2d-32          [-1, 128, 56, 56]         147,584
      BatchNorm2d-33          [-1, 128, 56, 56]             256
AdaptiveAvgPool2d-34            [-1, 128, 1, 1]               0
           Conv1d-35               [-1, 1, 128]               5
          Sigmoid-36            [-1, 128, 1, 1]               0
        eca_block-37          [-1, 128, 56, 56]               0
    ResidualBlock-38          [-1, 128, 56, 56]               0
           Conv2d-39          [-1, 128, 56, 56]         147,584
      BatchNorm2d-40          [-1, 128, 56, 56]             256
           Conv2d-41          [-1, 128, 56, 56]         147,584
      BatchNorm2d-42          [-1, 128, 56, 56]             256
AdaptiveAvgPool2d-43            [-1, 128, 1, 1]               0
           Conv1d-44               [-1, 1, 128]               5
          Sigmoid-45            [-1, 128, 1, 1]               0
        eca_block-46          [-1, 128, 56, 56]               0
    ResidualBlock-47          [-1, 128, 56, 56]               0
           Conv2d-48          [-1, 128, 56, 56]         147,584
      BatchNorm2d-49          [-1, 128, 56, 56]             256
           Conv2d-50          [-1, 128, 56, 56]         147,584
      BatchNorm2d-51          [-1, 128, 56, 56]             256
AdaptiveAvgPool2d-52            [-1, 128, 1, 1]               0
           Conv1d-53               [-1, 1, 128]               5
          Sigmoid-54            [-1, 128, 1, 1]               0
        eca_block-55          [-1, 128, 56, 56]               0
    ResidualBlock-56          [-1, 128, 56, 56]               0
           Conv2d-57          [-1, 128, 56, 56]         147,584
      BatchNorm2d-58          [-1, 128, 56, 56]             256
           Conv2d-59          [-1, 128, 56, 56]         147,584
      BatchNorm2d-60          [-1, 128, 56, 56]             256
AdaptiveAvgPool2d-61            [-1, 128, 1, 1]               0
           Conv1d-62               [-1, 1, 128]               5
          Sigmoid-63            [-1, 128, 1, 1]               0
        eca_block-64          [-1, 128, 56, 56]               0
    ResidualBlock-65          [-1, 128, 56, 56]               0
           Conv2d-66           [-1, 64, 56, 56]          16,384
      BatchNorm2d-67           [-1, 64, 56, 56]             128
      ConvBNLayer-68           [-1, 64, 56, 56]               0
           Conv2d-69           [-1, 64, 56, 56]          16,384
      BatchNorm2d-70           [-1, 64, 56, 56]             128
      ConvBNLayer-71           [-1, 64, 56, 56]               0
           Conv2d-72           [-1, 64, 56, 56]           4,096
      BatchNorm2d-73           [-1, 64, 56, 56]             128
      ConvBNLayer-74           [-1, 64, 56, 56]               0
           Conv2d-75           [-1, 64, 56, 56]          36,864
      BatchNorm2d-76           [-1, 64, 56, 56]             128
      ConvBNLayer-77           [-1, 64, 56, 56]               0
DarknetBottleneck-78           [-1, 64, 56, 56]               0
           Conv2d-79          [-1, 128, 56, 56]          16,384
      BatchNorm2d-80          [-1, 128, 56, 56]             256
      ConvBNLayer-81          [-1, 128, 56, 56]               0
         CSPLayer-82          [-1, 128, 56, 56]               0
AdaptiveAvgPool2d-83            [-1, 128, 1, 1]               0
           Conv1d-84               [-1, 1, 128]               5
          Sigmoid-85            [-1, 128, 1, 1]               0
        eca_block-86          [-1, 128, 56, 56]               0
           Conv2d-87            [-1, 1, 56, 56]              18
          Sigmoid-88            [-1, 1, 56, 56]               0
 SpatialAttention-89            [-1, 1, 56, 56]               0
       cbam_block-90          [-1, 128, 56, 56]               0
  ConvTranspose2d-91         [-1, 64, 112, 112]         131,136
      BatchNorm2d-92         [-1, 64, 112, 112]             128
           Conv2d-93         [-1, 32, 112, 112]           4,096
      BatchNorm2d-94         [-1, 32, 112, 112]              64
      ConvBNLayer-95         [-1, 32, 112, 112]               0
           Conv2d-96         [-1, 32, 112, 112]           4,096
      BatchNorm2d-97         [-1, 32, 112, 112]              64
      ConvBNLayer-98         [-1, 32, 112, 112]               0
           Conv2d-99         [-1, 32, 112, 112]           1,024
     BatchNorm2d-100         [-1, 32, 112, 112]              64
     ConvBNLayer-101         [-1, 32, 112, 112]               0
          Conv2d-102         [-1, 32, 112, 112]           9,216
     BatchNorm2d-103         [-1, 32, 112, 112]              64
     ConvBNLayer-104         [-1, 32, 112, 112]               0
DarknetBottleneck-105         [-1, 32, 112, 112]               0
          Conv2d-106         [-1, 64, 112, 112]           4,096
     BatchNorm2d-107         [-1, 64, 112, 112]             128
     ConvBNLayer-108         [-1, 64, 112, 112]               0
        CSPLayer-109         [-1, 64, 112, 112]               0
AdaptiveAvgPool2d-110             [-1, 64, 1, 1]               0
          Conv1d-111                [-1, 1, 64]               3
         Sigmoid-112             [-1, 64, 1, 1]               0
       eca_block-113         [-1, 64, 112, 112]               0
          Conv2d-114          [-1, 1, 112, 112]              18
         Sigmoid-115          [-1, 1, 112, 112]               0
SpatialAttention-116          [-1, 1, 112, 112]               0
      cbam_block-117         [-1, 64, 112, 112]               0
 ConvTranspose2d-118         [-1, 32, 224, 224]          32,800
     BatchNorm2d-119         [-1, 32, 224, 224]              64
          Conv2d-120         [-1, 16, 224, 224]           1,024
     BatchNorm2d-121         [-1, 16, 224, 224]              32
     ConvBNLayer-122         [-1, 16, 224, 224]               0
          Conv2d-123         [-1, 16, 224, 224]           1,024
     BatchNorm2d-124         [-1, 16, 224, 224]              32
     ConvBNLayer-125         [-1, 16, 224, 224]               0
          Conv2d-126         [-1, 16, 224, 224]             256
     BatchNorm2d-127         [-1, 16, 224, 224]              32
     ConvBNLayer-128         [-1, 16, 224, 224]               0
          Conv2d-129         [-1, 16, 224, 224]           2,304
     BatchNorm2d-130         [-1, 16, 224, 224]              32
     ConvBNLayer-131         [-1, 16, 224, 224]               0
DarknetBottleneck-132         [-1, 16, 224, 224]               0
          Conv2d-133         [-1, 32, 224, 224]           1,024
     BatchNorm2d-134         [-1, 32, 224, 224]              64
     ConvBNLayer-135         [-1, 32, 224, 224]               0
        CSPLayer-136         [-1, 32, 224, 224]               0
AdaptiveAvgPool2d-137             [-1, 32, 1, 1]               0
          Conv1d-138                [-1, 1, 32]               3
         Sigmoid-139             [-1, 32, 1, 1]               0
       eca_block-140         [-1, 32, 224, 224]               0
          Conv2d-141          [-1, 1, 224, 224]              18
         Sigmoid-142          [-1, 1, 224, 224]               0
SpatialAttention-143          [-1, 1, 224, 224]               0
      cbam_block-144         [-1, 32, 224, 224]               0
 ConvTranspose2d-145         [-1, 32, 224, 224]          82,976
         Dropout-146         [-1, 32, 224, 224]               0
          Conv2d-147          [-1, 1, 224, 224]              33
         Dropout-148         [-1, 32, 224, 224]               0
          Conv2d-149          [-1, 1, 224, 224]              33
         Dropout-150         [-1, 32, 224, 224]               0
          Conv2d-151          [-1, 1, 224, 224]              33
         Dropout-152         [-1, 32, 224, 224]               0
          Conv2d-153          [-1, 1, 224, 224]              33
================================================================
Total params: 1,867,486
Trainable params: 1,867,486
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.77
Forward/backward pass size (MB): 549.71
Params size (MB): 7.12
Estimated Total Size (MB): 557.60
----------------------------------------------------------------

import torch.nn as nn
import torch
import torch.nn.functional as F
from torch.nn.parameter import Parameter
from inference.models.pico_det import CSPLayer
from inference.models.grasp_model import GraspModel, ResidualBlock
from inference.models.pp_lcnet import DepthwiseSeparable

class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=8):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        # 利用1x1卷积代替全连接
        self.fc1   = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)
        self.relu1 = nn.ReLU()
        self.fc2   = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        out = avg_out + max_out
        return self.sigmoid(out)

import math
class eca_block(nn.Module):
    def __init__(self, channel, b=1, gamma=2):
        super(eca_block, self).__init__()
        kernel_size = int(abs((math.log(channel, 2) + b) / gamma))
        kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1
        
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False) 
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        y = self.avg_pool(x)
        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)
        y = self.sigmoid(y)
        return y.expand_as(x)


class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()

        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'
        padding = 3 if kernel_size == 7 else 1
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)

class cbam_block(nn.Module):
    def __init__(self, channel, ratio=8, kernel_size=3):
        super(cbam_block, self).__init__()
        self.channelattention = eca_block(channel =channel )
        self.spatialattention = SpatialAttention(kernel_size=kernel_size)

    def forward(self, x):
        x0 = self.channelattention(x)
        x1 = self.spatialattention(x)
        x = x0 * x * x1
        return x

class sa_layer(nn.Module):
    """Constructs a Channel Spatial Group module.
    Args:
        k_size: Adaptive selection of kernel size
    """

    def __init__(self, channel, groups=64):
        super(sa_layer, self).__init__()
        self.groups = groups
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.cweight = Parameter(torch.zeros(1, channel // (2 * groups), 1, 1))
        self.cbias = Parameter(torch.ones(1, channel // (2 * groups), 1, 1))
        self.sweight = Parameter(torch.zeros(1, channel // (2 * groups), 1, 1))
        self.sbias = Parameter(torch.ones(1, channel // (2 * groups), 1, 1))

        self.sigmoid = nn.Sigmoid()
        self.gn = nn.GroupNorm(channel // (2 * groups), channel // (2 * groups))

    @staticmethod
    def channel_shuffle(x, groups):
        b, c, h, w = x.shape

        x = x.reshape(b, groups, -1, h, w)
        x = x.permute(0, 2, 1, 3, 4)

        # flatten
        x = x.reshape(b, -1, h, w)

        return x

    def forward(self, x):
        b, c, h, w = x.shape
        print("shape x {}".format(x.shape))
        x = x.reshape(b * self.groups, -1, h, w)
        print("shape x {}".format(x.shape))
        x_0, x_1 = x.chunk(2, dim=1)

        # channel attention
        xn = self.avg_pool(x_0)
        xn = self.cweight * xn + self.cbias
        xn = x_0 * self.sigmoid(xn)

        # spatial attention
        xs = self.gn(x_1)
        xs = self.sweight * xs + self.sbias
        xs = x_1 * self.sigmoid(xs)

        # concatenate along channel axis
        out = torch.cat([xn, xs], dim=1)
        out = out.reshape(b, -1, h, w)

        out = self.channel_shuffle(out, 2)
        return out


class GenerativeResnet(GraspModel):

    def __init__(self, input_channels=4, output_channels=1, channel_size=32, dropout=False, prob=0.0):
        super(GenerativeResnet, self).__init__()
        
        self.conv1 = nn.Conv2d(input_channels, channel_size, kernel_size=9, stride=1, padding=4)  #32 224
        # self.conv1 = DepthwiseSeparable(input_channels,channel_size,stride = 1,use_se=True)
        self.bn1 = nn.BatchNorm2d(channel_size)
        self.att_1 = cbam_block(channel_size)

        # self.conv2 = nn.Conv2d(channel_size, channel_size * 2, kernel_size=4, stride=2, padding=1) #64 112
        self.conv2 = DepthwiseSeparable(channel_size,channel_size * 2,stride = 2,use_se=False)
        self.bn2 = nn.BatchNorm2d(channel_size * 2)
        self.att_2 = cbam_block(channel_size * 2)

        # self.conv3 = nn.Conv2d(channel_size * 2, channel_size * 4, kernel_size=4, stride=2, padding=1) #128 64
        self.conv3 = DepthwiseSeparable(channel_size * 2,channel_size * 4,stride = 2,use_se=False)
        self.bn3 = nn.BatchNorm2d(channel_size * 4)
        self.att_3 = cbam_block(channel_size * 4)

        # self.res1 = DepthwiseSeparable(channel_size * 4, channel_size * 4,dw_size=5,use_se=True)
        # self.res2 = DepthwiseSeparable(channel_size * 4, channel_size * 4,dw_size=5,use_se=True)
        # self.res3 = DepthwiseSeparable(channel_size * 4, channel_size * 4,dw_size=5,use_se=True)
        # self.res4 = DepthwiseSeparable(channel_size * 4, channel_size * 4,dw_size=5,use_se=True)
        # self.res5 = DepthwiseSeparable(channel_size * 4, channel_size * 4,dw_size=5,use_se=True)

        self.res1 = ResidualBlock(channel_size * 4, channel_size * 4, use_att=True) #128 56
        self.res2 = ResidualBlock(channel_size * 4, channel_size * 4, use_att=True) #128 56
        self.res3 = ResidualBlock(channel_size * 4, channel_size * 4, use_att=True) #128 56
        self.res4 = ResidualBlock(channel_size * 4, channel_size * 4, use_att=True) #128 56
        self.res5 = ResidualBlock(channel_size * 4, channel_size * 4, use_att=True) #128 56
        
        self.csp_layer_1 = CSPLayer(channel_size * 4 + channel_size * 4,channel_size * 4)
        self.sa_layer_1 = cbam_block(channel_size * 4)
        self.conv4 = nn.ConvTranspose2d(channel_size * 4, channel_size * 2, kernel_size=4, stride=2, padding=1) 
        self.bn4 = nn.BatchNorm2d(channel_size * 2)
        self.csp_layer_2 = CSPLayer(channel_size * 2 + channel_size * 2,channel_size*2)
        self.sa_layer_2 = cbam_block(channel_size * 2)
        self.conv5 = nn.ConvTranspose2d(channel_size * 2, channel_size, kernel_size=4, stride=2, padding=1)
        self.bn5 = nn.BatchNorm2d(channel_size)

        self.csp_layer_3 = CSPLayer(channel_size  + channel_size ,channel_size )
        self.sa_layer_3 = cbam_block(channel_size * 1)
        self.conv6 = nn.ConvTranspose2d(channel_size, channel_size, kernel_size=9, stride=1, padding=4)

        self.pos_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=1)
        self.cos_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=1)
        self.sin_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=1)
        self.width_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=1)

        self.dropout = dropout
        self.dropout_pos = nn.Dropout(p=prob)
        self.dropout_cos = nn.Dropout(p=prob)
        self.dropout_sin = nn.Dropout(p=prob)
        self.dropout_wid = nn.Dropout(p=prob)

        for m in self.modules():
            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
                nn.init.xavier_uniform_(m.weight, gain=1)
        
    def forward(self, x_in):
        dbg=0
        # x1 = self.conv1(x_in)
        x1 = F.hardswish(self.bn1(self.conv1(x_in))) #32
        if dbg == 1:
            print('x1.shape  {}'.format(x1.shape))
        x2 = self.conv2(x1)
        if dbg == 1:
            print('x2.shape  {}'.format(x2.shape))
        x3 = self.conv3(x2)
        if dbg == 1:
            print('x3.shape  {}'.format(x3.shape))
        xx = self.res1(x3)
        xx = self.res2(xx)
        xx = self.res3(xx)
        xx = self.res4(xx)
        x4 = self.res5(xx)
        if dbg == 1:
            print('xx.shape  {}'.format(xx.shape))

        x43 = self.csp_layer_1(torch.cat((x4, x3), dim=1))  #128+128 128
        if dbg == 1:
            print('csp x43.shape  {}'.format(x43.shape))
        x43 = self.sa_layer_1(x43)
        if dbg == 1:
            print('cbam x43.shape  {}'.format(x43.shape))
        x43 = F.hardswish(self.bn4(self.conv4(x43)))
        if dbg == 1:
            print('x43.shape  {}'.format(x43.shape))

        x42 = self.csp_layer_2(torch.cat((x43, x2), dim=1))  #64+64 64
        if dbg == 1:
            print('csp x42.shape  {}'.format(x42.shape))
        x42 = self.sa_layer_2(x42)
        if dbg == 1:
            print('cbam x42.shape  {}'.format(x42.shape))
        x42 = F.hardswish(self.bn5(self.conv5(x42)))
        
        if dbg == 1:
            print('x42.shape  {}'.format(x42.shape))

        x41 = self.csp_layer_3(torch.cat((x42, x1), dim=1))  #32+32 32
        x41 = self.sa_layer_3(x41)
        if dbg == 1:
            print('csp x41.shape  {}'.format(x41.shape))
        x = self.conv6(x41)
        if dbg == 1:
            print('x.shape  {}'.format(x.shape))
        if self.dropout:
            pos_output = self.pos_output(self.dropout_pos(x))
            cos_output = self.cos_output(self.dropout_cos(x))
            sin_output = self.sin_output(self.dropout_sin(x))
            width_output = self.width_output(self.dropout_wid(x))
        else:
            pos_output = self.pos_output(x)
            cos_output = self.cos_output(x)
            sin_output = self.sin_output(x)
            width_output = self.width_output(x)
        if dbg == 1:
            print('pos_output.shape  {}'.format(pos_output.shape))
        return pos_output, cos_output, sin_output, width_output
if __name__ == "__main__":
    model = GenerativeResnet()
    model.eval()
    input = torch.rand(1, 4, 224, 224)
    output = model(input)
