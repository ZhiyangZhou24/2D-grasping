----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 32, 320, 320]             288
       BatchNorm2d-2         [-1, 32, 320, 320]              64
         Hardswish-3         [-1, 32, 320, 320]               0
       ConvBNLayer-4         [-1, 32, 320, 320]               0
            Conv2d-5         [-1, 32, 320, 320]             288
       BatchNorm2d-6         [-1, 32, 320, 320]              64
         Hardswish-7         [-1, 32, 320, 320]               0
       ConvBNLayer-8         [-1, 32, 320, 320]               0
 AdaptiveAvgPool2d-9             [-1, 32, 1, 1]               0
           Conv2d-10              [-1, 8, 1, 1]             264
             ReLU-11              [-1, 8, 1, 1]               0
           Conv2d-12             [-1, 32, 1, 1]             288
      Hardsigmoid-13             [-1, 32, 1, 1]               0
         SEModule-14         [-1, 32, 320, 320]               0
           Conv2d-15         [-1, 32, 320, 320]           1,024
      BatchNorm2d-16         [-1, 32, 320, 320]              64
        Hardswish-17         [-1, 32, 320, 320]               0
      ConvBNLayer-18         [-1, 32, 320, 320]               0
DepthwiseSeparable-19         [-1, 32, 320, 320]               0
           Conv2d-20         [-1, 32, 160, 160]             288
      BatchNorm2d-21         [-1, 32, 160, 160]              64
        Hardswish-22         [-1, 32, 160, 160]               0
      ConvBNLayer-23         [-1, 32, 160, 160]               0
AdaptiveAvgPool2d-24             [-1, 32, 1, 1]               0
           Conv2d-25              [-1, 8, 1, 1]             264
             ReLU-26              [-1, 8, 1, 1]               0
           Conv2d-27             [-1, 32, 1, 1]             288
      Hardsigmoid-28             [-1, 32, 1, 1]               0
         SEModule-29         [-1, 32, 160, 160]               0
           Conv2d-30         [-1, 64, 160, 160]           2,048
      BatchNorm2d-31         [-1, 64, 160, 160]             128
        Hardswish-32         [-1, 64, 160, 160]               0
      ConvBNLayer-33         [-1, 64, 160, 160]               0
DepthwiseSeparable-34         [-1, 64, 160, 160]               0
           Conv2d-35         [-1, 64, 160, 160]             576
      BatchNorm2d-36         [-1, 64, 160, 160]             128
        Hardswish-37         [-1, 64, 160, 160]               0
      ConvBNLayer-38         [-1, 64, 160, 160]               0
AdaptiveAvgPool2d-39             [-1, 64, 1, 1]               0
           Conv2d-40             [-1, 16, 1, 1]           1,040
             ReLU-41             [-1, 16, 1, 1]               0
           Conv2d-42             [-1, 64, 1, 1]           1,088
      Hardsigmoid-43             [-1, 64, 1, 1]               0
         SEModule-44         [-1, 64, 160, 160]               0
           Conv2d-45         [-1, 64, 160, 160]           4,096
      BatchNorm2d-46         [-1, 64, 160, 160]             128
        Hardswish-47         [-1, 64, 160, 160]               0
      ConvBNLayer-48         [-1, 64, 160, 160]               0
DepthwiseSeparable-49         [-1, 64, 160, 160]               0
           Conv2d-50           [-1, 64, 80, 80]             576
      BatchNorm2d-51           [-1, 64, 80, 80]             128
        Hardswish-52           [-1, 64, 80, 80]               0
      ConvBNLayer-53           [-1, 64, 80, 80]               0
AdaptiveAvgPool2d-54             [-1, 64, 1, 1]               0
           Conv2d-55             [-1, 16, 1, 1]           1,040
             ReLU-56             [-1, 16, 1, 1]               0
           Conv2d-57             [-1, 64, 1, 1]           1,088
      Hardsigmoid-58             [-1, 64, 1, 1]               0
         SEModule-59           [-1, 64, 80, 80]               0
           Conv2d-60          [-1, 128, 80, 80]           8,192
      BatchNorm2d-61          [-1, 128, 80, 80]             256
        Hardswish-62          [-1, 128, 80, 80]               0
      ConvBNLayer-63          [-1, 128, 80, 80]               0
DepthwiseSeparable-64          [-1, 128, 80, 80]               0
           Conv2d-65          [-1, 128, 80, 80]           1,152
      BatchNorm2d-66          [-1, 128, 80, 80]             256
        Hardswish-67          [-1, 128, 80, 80]               0
      ConvBNLayer-68          [-1, 128, 80, 80]               0
AdaptiveAvgPool2d-69            [-1, 128, 1, 1]               0
           Conv2d-70             [-1, 32, 1, 1]           4,128
             ReLU-71             [-1, 32, 1, 1]               0
           Conv2d-72            [-1, 128, 1, 1]           4,224
      Hardsigmoid-73            [-1, 128, 1, 1]               0
         SEModule-74          [-1, 128, 80, 80]               0
           Conv2d-75          [-1, 128, 80, 80]          16,384
      BatchNorm2d-76          [-1, 128, 80, 80]             256
        Hardswish-77          [-1, 128, 80, 80]               0
      ConvBNLayer-78          [-1, 128, 80, 80]               0
DepthwiseSeparable-79          [-1, 128, 80, 80]               0
           Conv2d-80          [-1, 128, 40, 40]           1,152
      BatchNorm2d-81          [-1, 128, 40, 40]             256
        Hardswish-82          [-1, 128, 40, 40]               0
      ConvBNLayer-83          [-1, 128, 40, 40]               0
AdaptiveAvgPool2d-84            [-1, 128, 1, 1]               0
           Conv2d-85             [-1, 32, 1, 1]           4,128
             ReLU-86             [-1, 32, 1, 1]               0
           Conv2d-87            [-1, 128, 1, 1]           4,224
      Hardsigmoid-88            [-1, 128, 1, 1]               0
         SEModule-89          [-1, 128, 40, 40]               0
           Conv2d-90          [-1, 256, 40, 40]          32,768
      BatchNorm2d-91          [-1, 256, 40, 40]             512
        Hardswish-92          [-1, 256, 40, 40]               0
      ConvBNLayer-93          [-1, 256, 40, 40]               0
DepthwiseSeparable-94          [-1, 256, 40, 40]               0
           Conv2d-95          [-1, 256, 40, 40]           6,400
      BatchNorm2d-96          [-1, 256, 40, 40]             512
        Hardswish-97          [-1, 256, 40, 40]               0
      ConvBNLayer-98          [-1, 256, 40, 40]               0
AdaptiveAvgPool2d-99            [-1, 256, 1, 1]               0
          Conv2d-100             [-1, 64, 1, 1]          16,448
            ReLU-101             [-1, 64, 1, 1]               0
          Conv2d-102            [-1, 256, 1, 1]          16,640
     Hardsigmoid-103            [-1, 256, 1, 1]               0
        SEModule-104          [-1, 256, 40, 40]               0
          Conv2d-105          [-1, 256, 40, 40]          65,536
     BatchNorm2d-106          [-1, 256, 40, 40]             512
       Hardswish-107          [-1, 256, 40, 40]               0
     ConvBNLayer-108          [-1, 256, 40, 40]               0
DepthwiseSeparable-109          [-1, 256, 40, 40]               0
          Conv2d-110          [-1, 256, 40, 40]           6,400
     BatchNorm2d-111          [-1, 256, 40, 40]             512
       Hardswish-112          [-1, 256, 40, 40]               0
     ConvBNLayer-113          [-1, 256, 40, 40]               0
AdaptiveAvgPool2d-114            [-1, 256, 1, 1]               0
          Conv2d-115             [-1, 64, 1, 1]          16,448
            ReLU-116             [-1, 64, 1, 1]               0
          Conv2d-117            [-1, 256, 1, 1]          16,640
     Hardsigmoid-118            [-1, 256, 1, 1]               0
        SEModule-119          [-1, 256, 40, 40]               0
          Conv2d-120          [-1, 256, 40, 40]          65,536
     BatchNorm2d-121          [-1, 256, 40, 40]             512
       Hardswish-122          [-1, 256, 40, 40]               0
     ConvBNLayer-123          [-1, 256, 40, 40]               0
DepthwiseSeparable-124          [-1, 256, 40, 40]               0
          Conv2d-125          [-1, 256, 40, 40]           6,400
     BatchNorm2d-126          [-1, 256, 40, 40]             512
       Hardswish-127          [-1, 256, 40, 40]               0
     ConvBNLayer-128          [-1, 256, 40, 40]               0
AdaptiveAvgPool2d-129            [-1, 256, 1, 1]               0
          Conv2d-130             [-1, 64, 1, 1]          16,448
            ReLU-131             [-1, 64, 1, 1]               0
          Conv2d-132            [-1, 256, 1, 1]          16,640
     Hardsigmoid-133            [-1, 256, 1, 1]               0
        SEModule-134          [-1, 256, 40, 40]               0
          Conv2d-135          [-1, 256, 40, 40]          65,536
     BatchNorm2d-136          [-1, 256, 40, 40]             512
       Hardswish-137          [-1, 256, 40, 40]               0
     ConvBNLayer-138          [-1, 256, 40, 40]               0
DepthwiseSeparable-139          [-1, 256, 40, 40]               0
          Conv2d-140          [-1, 256, 40, 40]           6,400
     BatchNorm2d-141          [-1, 256, 40, 40]             512
       Hardswish-142          [-1, 256, 40, 40]               0
     ConvBNLayer-143          [-1, 256, 40, 40]               0
AdaptiveAvgPool2d-144            [-1, 256, 1, 1]               0
          Conv2d-145             [-1, 64, 1, 1]          16,448
            ReLU-146             [-1, 64, 1, 1]               0
          Conv2d-147            [-1, 256, 1, 1]          16,640
     Hardsigmoid-148            [-1, 256, 1, 1]               0
        SEModule-149          [-1, 256, 40, 40]               0
          Conv2d-150          [-1, 256, 40, 40]          65,536
     BatchNorm2d-151          [-1, 256, 40, 40]             512
       Hardswish-152          [-1, 256, 40, 40]               0
     ConvBNLayer-153          [-1, 256, 40, 40]               0
DepthwiseSeparable-154          [-1, 256, 40, 40]               0
          Conv2d-155          [-1, 256, 40, 40]           6,400
     BatchNorm2d-156          [-1, 256, 40, 40]             512
       Hardswish-157          [-1, 256, 40, 40]               0
     ConvBNLayer-158          [-1, 256, 40, 40]               0
AdaptiveAvgPool2d-159            [-1, 256, 1, 1]               0
          Conv2d-160             [-1, 64, 1, 1]          16,448
            ReLU-161             [-1, 64, 1, 1]               0
          Conv2d-162            [-1, 256, 1, 1]          16,640
     Hardsigmoid-163            [-1, 256, 1, 1]               0
        SEModule-164          [-1, 256, 40, 40]               0
          Conv2d-165          [-1, 256, 40, 40]          65,536
     BatchNorm2d-166          [-1, 256, 40, 40]             512
       Hardswish-167          [-1, 256, 40, 40]               0
     ConvBNLayer-168          [-1, 256, 40, 40]               0
DepthwiseSeparable-169          [-1, 256, 40, 40]               0
          Conv2d-170           [-1, 64, 40, 40]          32,768
     BatchNorm2d-171           [-1, 64, 40, 40]             128
     ConvBNLayer-172           [-1, 64, 40, 40]               0
          Conv2d-173           [-1, 64, 40, 40]          32,768
     BatchNorm2d-174           [-1, 64, 40, 40]             128
     ConvBNLayer-175           [-1, 64, 40, 40]               0
          Conv2d-176           [-1, 64, 40, 40]           4,096
     BatchNorm2d-177           [-1, 64, 40, 40]             128
     ConvBNLayer-178           [-1, 64, 40, 40]               0
          Conv2d-179           [-1, 64, 40, 40]          36,864
     BatchNorm2d-180           [-1, 64, 40, 40]             128
     ConvBNLayer-181           [-1, 64, 40, 40]               0
DarknetBottleneck-182           [-1, 64, 40, 40]               0
          Conv2d-183           [-1, 64, 40, 40]           4,096
     BatchNorm2d-184           [-1, 64, 40, 40]             128
     ConvBNLayer-185           [-1, 64, 40, 40]               0
          Conv2d-186           [-1, 64, 40, 40]          36,864
     BatchNorm2d-187           [-1, 64, 40, 40]             128
     ConvBNLayer-188           [-1, 64, 40, 40]               0
DarknetBottleneck-189           [-1, 64, 40, 40]               0
          Conv2d-190          [-1, 128, 40, 40]          16,384
     BatchNorm2d-191          [-1, 128, 40, 40]             256
     ConvBNLayer-192          [-1, 128, 40, 40]               0
        CSPLayer-193          [-1, 128, 40, 40]               0
        Upsample-194          [-1, 128, 80, 80]               0
              up-195          [-1, 128, 80, 80]               0
          Conv2d-196           [-1, 32, 80, 80]           8,192
     BatchNorm2d-197           [-1, 32, 80, 80]              64
     ConvBNLayer-198           [-1, 32, 80, 80]               0
          Conv2d-199           [-1, 32, 80, 80]           8,192
     BatchNorm2d-200           [-1, 32, 80, 80]              64
     ConvBNLayer-201           [-1, 32, 80, 80]               0
          Conv2d-202           [-1, 32, 80, 80]           1,024
     BatchNorm2d-203           [-1, 32, 80, 80]              64
     ConvBNLayer-204           [-1, 32, 80, 80]               0
          Conv2d-205           [-1, 32, 80, 80]           9,216
     BatchNorm2d-206           [-1, 32, 80, 80]              64
     ConvBNLayer-207           [-1, 32, 80, 80]               0
DarknetBottleneck-208           [-1, 32, 80, 80]               0
          Conv2d-209           [-1, 32, 80, 80]           1,024
     BatchNorm2d-210           [-1, 32, 80, 80]              64
     ConvBNLayer-211           [-1, 32, 80, 80]               0
          Conv2d-212           [-1, 32, 80, 80]           9,216
     BatchNorm2d-213           [-1, 32, 80, 80]              64
     ConvBNLayer-214           [-1, 32, 80, 80]               0
DarknetBottleneck-215           [-1, 32, 80, 80]               0
          Conv2d-216           [-1, 64, 80, 80]           4,096
     BatchNorm2d-217           [-1, 64, 80, 80]             128
     ConvBNLayer-218           [-1, 64, 80, 80]               0
        CSPLayer-219           [-1, 64, 80, 80]               0
        Upsample-220         [-1, 64, 160, 160]               0
              up-221         [-1, 64, 160, 160]               0
          Conv2d-222         [-1, 16, 160, 160]           2,048
     BatchNorm2d-223         [-1, 16, 160, 160]              32
     ConvBNLayer-224         [-1, 16, 160, 160]               0
          Conv2d-225         [-1, 16, 160, 160]           2,048
     BatchNorm2d-226         [-1, 16, 160, 160]              32
     ConvBNLayer-227         [-1, 16, 160, 160]               0
          Conv2d-228         [-1, 16, 160, 160]             256
     BatchNorm2d-229         [-1, 16, 160, 160]              32
     ConvBNLayer-230         [-1, 16, 160, 160]               0
          Conv2d-231         [-1, 16, 160, 160]           2,304
     BatchNorm2d-232         [-1, 16, 160, 160]              32
     ConvBNLayer-233         [-1, 16, 160, 160]               0
DarknetBottleneck-234         [-1, 16, 160, 160]               0
          Conv2d-235         [-1, 16, 160, 160]             256
     BatchNorm2d-236         [-1, 16, 160, 160]              32
     ConvBNLayer-237         [-1, 16, 160, 160]               0
          Conv2d-238         [-1, 16, 160, 160]           2,304
     BatchNorm2d-239         [-1, 16, 160, 160]              32
     ConvBNLayer-240         [-1, 16, 160, 160]               0
DarknetBottleneck-241         [-1, 16, 160, 160]               0
          Conv2d-242         [-1, 32, 160, 160]           1,024
     BatchNorm2d-243         [-1, 32, 160, 160]              64
     ConvBNLayer-244         [-1, 32, 160, 160]               0
        CSPLayer-245         [-1, 32, 160, 160]               0
        Upsample-246         [-1, 32, 320, 320]               0
              up-247         [-1, 32, 320, 320]               0
         Dropout-248         [-1, 32, 320, 320]               0
          Conv2d-249          [-1, 1, 320, 320]              33
         Dropout-250         [-1, 32, 320, 320]               0
          Conv2d-251          [-1, 1, 320, 320]              33
         Dropout-252         [-1, 32, 320, 320]               0
          Conv2d-253          [-1, 1, 320, 320]              33
         Dropout-254         [-1, 32, 320, 320]               0
          Conv2d-255          [-1, 1, 320, 320]              33
================================================================
Total params: 840,404
Trainable params: 840,404
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.39
Forward/backward pass size (MB): 1201.61
Params size (MB): 3.21
Estimated Total Size (MB): 1205.21
----------------------------------------------------------------
