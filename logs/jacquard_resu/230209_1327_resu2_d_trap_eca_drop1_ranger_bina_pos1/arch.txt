----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 32, 320, 320]             320
       BatchNorm2d-2         [-1, 32, 320, 320]              64
              ReLU-3         [-1, 32, 320, 320]               0
            Conv2d-4         [-1, 64, 160, 160]          18,496
       BatchNorm2d-5         [-1, 64, 160, 160]             128
              ReLU-6         [-1, 64, 160, 160]               0
 AdaptiveAvgPool2d-7             [-1, 64, 1, 1]               0
            Conv1d-8                [-1, 1, 64]               3
           Sigmoid-9             [-1, 64, 1, 1]               0
        eca_block-10         [-1, 64, 160, 160]               0
           inconv-11         [-1, 64, 160, 160]               0
        MaxPool2d-12           [-1, 64, 80, 80]               0
           Conv2d-13          [-1, 128, 80, 80]          73,856
      BatchNorm2d-14          [-1, 128, 80, 80]             256
             ReLU-15          [-1, 128, 80, 80]               0
AdaptiveAvgPool2d-16            [-1, 128, 1, 1]               0
           Conv1d-17               [-1, 1, 128]               3
          Sigmoid-18            [-1, 128, 1, 1]               0
        eca_block-19          [-1, 128, 80, 80]               0
           Conv2d-20          [-1, 128, 80, 80]           8,192
      BatchNorm2d-21          [-1, 128, 80, 80]             256
         conv_att-22          [-1, 128, 80, 80]               0
             down-23          [-1, 128, 80, 80]               0
        MaxPool2d-24          [-1, 128, 40, 40]               0
           Conv2d-25          [-1, 256, 40, 40]         295,168
      BatchNorm2d-26          [-1, 256, 40, 40]             512
             ReLU-27          [-1, 256, 40, 40]               0
AdaptiveAvgPool2d-28            [-1, 256, 1, 1]               0
           Conv1d-29               [-1, 1, 256]               5
          Sigmoid-30            [-1, 256, 1, 1]               0
        eca_block-31          [-1, 256, 40, 40]               0
           Conv2d-32          [-1, 256, 40, 40]          32,768
      BatchNorm2d-33          [-1, 256, 40, 40]             512
         conv_att-34          [-1, 256, 40, 40]               0
             down-35          [-1, 256, 40, 40]               0
        MaxPool2d-36          [-1, 256, 20, 20]               0
           Conv2d-37          [-1, 512, 20, 20]       1,180,160
      BatchNorm2d-38          [-1, 512, 20, 20]           1,024
             ReLU-39          [-1, 512, 20, 20]               0
AdaptiveAvgPool2d-40            [-1, 512, 1, 1]               0
           Conv1d-41               [-1, 1, 512]               5
          Sigmoid-42            [-1, 512, 1, 1]               0
        eca_block-43          [-1, 512, 20, 20]               0
           Conv2d-44          [-1, 512, 20, 20]         131,072
      BatchNorm2d-45          [-1, 512, 20, 20]           1,024
         conv_att-46          [-1, 512, 20, 20]               0
             down-47          [-1, 512, 20, 20]               0
        MaxPool2d-48          [-1, 512, 10, 10]               0
           Conv2d-49          [-1, 512, 10, 10]       2,359,808
      BatchNorm2d-50          [-1, 512, 10, 10]           1,024
             ReLU-51          [-1, 512, 10, 10]               0
AdaptiveAvgPool2d-52            [-1, 512, 1, 1]               0
           Conv1d-53               [-1, 1, 512]               5
          Sigmoid-54            [-1, 512, 1, 1]               0
        eca_block-55          [-1, 512, 10, 10]               0
         conv_att-56          [-1, 512, 10, 10]               0
             down-57          [-1, 512, 10, 10]               0
  ConvTranspose2d-58          [-1, 512, 20, 20]       1,049,088
           Conv2d-59          [-1, 256, 20, 20]       2,359,552
      BatchNorm2d-60          [-1, 256, 20, 20]             512
             ReLU-61          [-1, 256, 20, 20]               0
AdaptiveAvgPool2d-62            [-1, 256, 1, 1]               0
           Conv1d-63               [-1, 1, 256]               5
          Sigmoid-64            [-1, 256, 1, 1]               0
        eca_block-65          [-1, 256, 20, 20]               0
           Conv2d-66          [-1, 256, 20, 20]         262,144
      BatchNorm2d-67          [-1, 256, 20, 20]             512
         conv_att-68          [-1, 256, 20, 20]               0
               up-69          [-1, 256, 20, 20]               0
  ConvTranspose2d-70          [-1, 256, 40, 40]         262,400
           Conv2d-71          [-1, 128, 40, 40]         589,952
      BatchNorm2d-72          [-1, 128, 40, 40]             256
             ReLU-73          [-1, 128, 40, 40]               0
AdaptiveAvgPool2d-74            [-1, 128, 1, 1]               0
           Conv1d-75               [-1, 1, 128]               5
          Sigmoid-76            [-1, 128, 1, 1]               0
        eca_block-77          [-1, 128, 40, 40]               0
           Conv2d-78          [-1, 128, 40, 40]          65,536
      BatchNorm2d-79          [-1, 128, 40, 40]             256
         conv_att-80          [-1, 128, 40, 40]               0
               up-81          [-1, 128, 40, 40]               0
  ConvTranspose2d-82          [-1, 128, 80, 80]          65,664
           Conv2d-83           [-1, 64, 80, 80]         147,520
      BatchNorm2d-84           [-1, 64, 80, 80]             128
             ReLU-85           [-1, 64, 80, 80]               0
AdaptiveAvgPool2d-86             [-1, 64, 1, 1]               0
           Conv1d-87                [-1, 1, 64]               5
          Sigmoid-88             [-1, 64, 1, 1]               0
        eca_block-89           [-1, 64, 80, 80]               0
           Conv2d-90           [-1, 64, 80, 80]          16,384
      BatchNorm2d-91           [-1, 64, 80, 80]             128
         conv_att-92           [-1, 64, 80, 80]               0
               up-93           [-1, 64, 80, 80]               0
  ConvTranspose2d-94         [-1, 64, 160, 160]          16,448
           Conv2d-95         [-1, 64, 160, 160]          73,792
      BatchNorm2d-96         [-1, 64, 160, 160]             128
             ReLU-97         [-1, 64, 160, 160]               0
AdaptiveAvgPool2d-98             [-1, 64, 1, 1]               0
           Conv1d-99                [-1, 1, 64]               5
         Sigmoid-100             [-1, 64, 1, 1]               0
       eca_block-101         [-1, 64, 160, 160]               0
          Conv2d-102         [-1, 64, 160, 160]           8,192
     BatchNorm2d-103         [-1, 64, 160, 160]             128
        conv_att-104         [-1, 64, 160, 160]               0
              up-105         [-1, 64, 160, 160]               0
 ConvTranspose2d-106         [-1, 32, 320, 320]           8,224
     BatchNorm2d-107         [-1, 32, 320, 320]              64
            ReLU-108         [-1, 32, 320, 320]               0
        up_final-109         [-1, 32, 320, 320]               0
         Dropout-110         [-1, 32, 320, 320]               0
          Conv2d-111          [-1, 1, 320, 320]              33
         Dropout-112         [-1, 32, 320, 320]               0
          Conv2d-113          [-1, 1, 320, 320]              33
         Dropout-114         [-1, 32, 320, 320]               0
          Conv2d-115          [-1, 1, 320, 320]              33
         Dropout-116         [-1, 32, 320, 320]               0
          Conv2d-117          [-1, 1, 320, 320]              33
================================================================
Total params: 9,031,821
Trainable params: 9,031,821
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.39
Forward/backward pass size (MB): 603.56
Params size (MB): 34.45
Estimated Total Size (MB): 638.41
----------------------------------------------------------------

import torch
import torch.nn as nn
import torch.nn.functional as F
from inference.models.attention import CoordAtt, eca_block, se_block,cbam_block
from inference.models.grasp_model import GraspModel, Mish
from inference.models.duc import DenseUpsamplingConvolution
from torchsummary import summary

class conv_att(nn.Module):
    '''(conv => BN => ReLU) * 2'''

    def __init__(self, in_ch, out_ch,use_mish=False,att_type = 'use_eca', reduc_ratio=3):
        super(conv_att, self).__init__()
        self.att_type = att_type

        self.att = self._make_att(in_ch, in_ch,reduc_ratio)
        if use_mish:
            self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            Mish()
        )
        else:
            self.conv = nn.Sequential(
                nn.Conv2d(in_ch, out_ch, 3, padding=1),
                nn.BatchNorm2d(out_ch),
                nn.ReLU()
            )
        
        self.channel_conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, bias=False),
            nn.BatchNorm2d(out_ch),
	        # nn.GroupNorm(32, out_ch),
        )
    def _make_att(self, in_channels, out_channels,reduc_ratio):
        if self.att_type == 'use_coora':
            print('use_coora reduc_ratio = {}'.format(reduc_ratio))
            return CoordAtt(in_channels,out_channels,reduc_ratio)
        elif self.att_type == 'use_eca':
            print('use_eca reduc_ratio = {}'.format(reduc_ratio))
            return eca_block(out_channels)
        elif self.att_type == 'use_se':
            print('use_se reduc_ratio = {}'.format(reduc_ratio))
            return se_block(channel=out_channels,ratio = reduc_ratio)
        elif self.att_type == 'use_cbam':
            print('use_cbam reduc_ratio = {}'.format(reduc_ratio))
            return cbam_block(out_channels,ratio=reduc_ratio)
        else :
            print('att_type error , please check!!!!')

    def forward(self, x):
        residual = x
        x = self.conv(x)
        x = self.att(x)

        if residual.shape[1] != x.shape[1]:
            residual = self.channel_conv(residual)
        x += residual
        return x


class inconv(nn.Module):
    def __init__(self, in_channels, out_channels,use_mish=False,att_type=None, reduc_ratio = 16):
        super(inconv, self).__init__()
        if use_mish:
            self.conv = nn.Sequential(
                nn.Conv2d(in_channels, out_channels // 2, kernel_size=3, stride=1, padding=1),
                nn.BatchNorm2d(out_channels // 2),
                Mish(),
                nn.Conv2d(out_channels // 2, out_channels, kernel_size= 3, stride=2, padding=1),
                nn.BatchNorm2d(out_channels),
                Mish()
            )
        else:
            self.conv = nn.Sequential(
                nn.Conv2d(in_channels, out_channels // 2, kernel_size=3, stride=1, padding=1),
                nn.BatchNorm2d(out_channels // 2),
                nn.ReLU(),
                nn.Conv2d(out_channels // 2, out_channels,kernel_size= 3, stride=2, padding=1),
                nn.BatchNorm2d(out_channels),
                nn.ReLU()
            )
        
        if att_type == 'use_coora':
            self.att = CoordAtt(out_channels,out_channels,reduc_ratio)
        elif att_type == 'use_eca':
            print('use_eca reduc_ratio = {}'.format(reduc_ratio))
            self.att = eca_block(out_channels)
        elif att_type == 'use_se':
            print('use_se reduc_ratio = {}'.format(reduc_ratio))
            self.att = se_block(channel=out_channels,ratio = reduc_ratio)
        elif att_type == 'use_cbam':
            print('use_cbam reduc_ratio = {}'.format(reduc_ratio))
            self.att = cbam_block(out_channels,ratio=reduc_ratio)
        elif att_type == None :
            print('inconv do not use attention')
        else :
            print('att_type error , please check!!!!')
    def forward(self, x):
        x = self.conv(x)
        if self.att != None:
            x = self.att(x)
        return x


class down(nn.Module):
    def __init__(self, in_ch, out_ch,use_mish=False,att_type='use_eca'):
        super(down, self).__init__()
        self.mpconv = nn.Sequential(
            nn.MaxPool2d(kernel_size = 2),
            conv_att(in_ch, out_ch,att_type=att_type,use_mish=use_mish)
        )

    def forward(self, x):
        x = self.mpconv(x)
        return x


class up(nn.Module):
    def __init__(self, in_ch, out_ch,use_mish=False,att_type='use_eca',upsample_type='use_convt'):
        super(up, self).__init__()
        self.upsample_type = upsample_type
        self.att_type = att_type

        self.up = self._make_upconv(in_ch // 2 , in_ch // 2, upscale_factor = 2)

        self.conv = conv_att(in_ch, out_ch,att_type = att_type,use_mish=use_mish)
    def _make_upconv(self, in_channels, out_channels, upscale_factor = 2):
        if self.upsample_type == 'use_duc':
            print('duc')
            return DenseUpsamplingConvolution(in_channels, out_channels, upscale_factor = upscale_factor)
        elif self.upsample_type == 'use_convt':
            print('use_convt')
            return nn.Sequential(
                nn.ConvTranspose2d(in_channels, out_channels, 2, stride = upscale_factor)
            )
        elif self.upsample_type == 'use_bilinear':
            print('use_bilinear')
            return nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        else :
            print('upsample_type error , please check!!!!')

    def forward(self, x1, x2):
        x1 = self.up(x1)

        diffY = x2.size()[2] - x1.size()[2]
        diffX = x2.size()[3] - x1.size()[3]

        x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2,
                        diffY // 2, diffY - diffY // 2))

        x = torch.cat([x2, x1], dim=1)
        x = self.conv(x)
        return x

class up_final(nn.Module):
    def __init__(self, in_ch, out_ch, use_mish=False):
        super(up_final, self).__init__()

        if use_mish:
            self.up = nn.Sequential(
            nn.ConvTranspose2d(in_ch, out_ch, 2,stride = 2),
            nn.BatchNorm2d(out_ch),
            Mish()
        )
        else:
            self.up = nn.Sequential(
                nn.ConvTranspose2d(in_ch, out_ch, 2,stride = 2),
                nn.BatchNorm2d(out_ch),
                nn.ReLU()
            )


    def forward(self, x):
        x = self.up(x) #64
        return x

class GenerativeResnet(GraspModel):

    def __init__(self, input_channels=4, output_channels=1, channel_size=32,use_mish=False, att = 'use_eca',upsamp='use_convt', dropout=False, prob=0.0):
        super(GenerativeResnet, self).__init__()
        print('Model is resunet2')
        print('Model upsamp {}'.format(upsamp))
        print('Model att {}'.format(att))

        self.inconv = inconv(input_channels,channel_size * 2,use_mish=use_mish,att_type='use_eca')

        self.down1 = down(channel_size * 2, channel_size * 4, att_type=att,use_mish=use_mish)
        self.down2 = down(channel_size * 4, channel_size * 8, att_type=att,use_mish=use_mish)
        self.down3 = down(channel_size * 8, channel_size * 16, att_type=att,use_mish=use_mish)
        self.down4 = down(channel_size * 16, channel_size * 16, att_type=att,use_mish=use_mish)
        self.up1 = up(channel_size * 16 * 2, channel_size * 8, att_type=att,upsample_type=upsamp,use_mish=use_mish)
        self.up2 = up(channel_size * 8 * 2, channel_size * 4, att_type=att,upsample_type=upsamp,use_mish=use_mish)
        self.up3 = up(channel_size * 4 * 2, channel_size * 2, att_type=att,upsample_type=upsamp,use_mish=use_mish)
        self.up4 = up(channel_size * 2 * 2, channel_size * 2, att_type=att,upsample_type=upsamp,use_mish=use_mish)

        self.up_final = up_final(channel_size * 2, channel_size)

        self.pos_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=1)
        self.cos_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=1)
        self.sin_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=1)
        self.width_output = nn.Conv2d(in_channels=channel_size, out_channels=output_channels, kernel_size=1)

        self.dropout = dropout
        self.dropout_pos = nn.Dropout(p=prob)
        self.dropout_cos = nn.Dropout(p=prob)
        self.dropout_sin = nn.Dropout(p=prob)
        self.dropout_wid = nn.Dropout(p=prob)

        for m in self.modules():
            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
                nn.init.xavier_uniform_(m.weight, gain=1)

    def forward(self, x_in):
        dbg = 0
        x1 = self.inconv(x_in)
        if dbg == 1:
            print('x1.shape  {}'.format(x1.shape))

        x2 = self.down1(x1) 
        if dbg == 1:
            print('x2.shape  {}'.format(x2.shape))

        x3 = self.down2(x2) 
        if dbg == 1:
            print('x3.shape  {}'.format(x3.shape))

        x4 = self.down3(x3) 
        if dbg == 1:
            print('x4.shape  {}'.format(x4.shape))

        x5 = self.down4(x4) 
        if dbg == 1:
            print('x5.shape  {}'.format(x5.shape))

        x44 = self.up1(x5, x4) #512 + 512 256 14
        if dbg == 1:
            print('x44.shape  {}'.format(x44.shape))
        x33 = self.up2(x44,x3) #256 +256 > 128 28
        if dbg == 1:
            print('x33.shape  {}'.format(x33.shape))
        x22 = self.up3(x33,x2) # 128 + 128 > 64 56
        if dbg == 1:
            print('x22.shape  {}'.format(x22.shape))
        x11 = self.up4(x22,x1) #64 112
        if dbg == 1:
            print('x11.shape  {}'.format(x11.shape))

        # x = F.relu(self.bn5(self.conv5(self.up_out(x11)))) #32 224
        x = self.up_final(x11)
        if dbg == 1:
            print('x.shape  {}'.format(x.shape))

        if self.dropout:
            pos_output = self.pos_output(self.dropout_pos(x))
            cos_output = self.cos_output(self.dropout_cos(x))
            sin_output = self.sin_output(self.dropout_sin(x))
            width_output = self.width_output(self.dropout_wid(x))
        else:
            pos_output = self.pos_output(x)
            cos_output = self.cos_output(x)
            sin_output = self.sin_output(x)
            width_output = self.width_output(x)
        return pos_output, cos_output, sin_output, width_output

import sys
sys.path.append('/home/lab/zzy/grasp/2D-grasping-my')
if __name__ == '__main__':
    model = GenerativeResnet()
    model.eval()
    input = torch.rand(1, 4, 224, 224)
    summary(model, (4, 224, 224),device='cpu')
    sys.stdout = sys.__stdout__
    output = model(input)
